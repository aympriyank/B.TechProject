{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"NYLFieC9yN1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/util')"],"metadata":{"id":"tvr7mmZ6z_ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","os.listdir()"],"metadata":{"id":"3H5l5XEZzeHn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#LOAD DATASET"],"metadata":{"id":"UhSY1RJw60Yb"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"khteXceKyIwC","executionInfo":{"status":"ok","timestamp":1682317168983,"user_tz":-330,"elapsed":9731,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}}},"outputs":[],"source":["import json\n","import os\n","\n","import networkx as nx\n","from networkx.readwrite import json_graph\n","\n","from util import constants\n","from util.util import tweet_node\n","\n","\n","def construct_tweet_node_from_json(json_data):\n","    new_graph = json_graph.tree_graph(json_data)\n","    root_node = 0\n","    print(new_graph)\n","    #print(nx.DiGraph.in_degree(new_graph))\n","    # root_node = [node for node, in_degree in nx.DiGraph.in_degree(new_graph).items() if in_degree == 0][0]\n","    node_id_obj_dict = dict()\n","    dfs_node_construction_helper(root_node, new_graph, set(), node_id_obj_dict)\n","    return node_id_obj_dict[root_node]\n","\n","\n","def dfs_node_construction_helper(node_id, graph: nx.DiGraph, visited: set, node_id_obj_dict: dict):\n","    if node_id in visited:\n","        return None\n","\n","    visited.add(node_id)\n","\n","    tweet_node_obj = construct_tweet_node_from_nx_node(node_id, graph)\n","\n","    node_id_obj_dict[node_id] = tweet_node_obj\n","\n","    for neighbor_node_id in graph.successors(node_id):\n","        if neighbor_node_id not in visited:\n","            dfs_node_construction_helper(neighbor_node_id, graph, visited, node_id_obj_dict)\n","            add_node_object_edge(node_id, neighbor_node_id, node_id_obj_dict)\n","\n","\n","def add_node_object_edge(parent_node_id: int, child_node_id: int, node_id_obj_dict: dict):\n","    parent_node = node_id_obj_dict[parent_node_id]\n","    child_node = node_id_obj_dict[child_node_id]\n","\n","    if child_node.node_type == constants.RETWEET_NODE:\n","        parent_node.add_retweet_child(child_node)\n","    elif child_node.node_type == constants.REPLY_NODE:\n","        parent_node.add_reply_child(child_node)\n","    else:\n","        # news node add both retweet and reply edge\n","        parent_node.add_retweet_child(child_node)\n","        parent_node.add_reply_child(child_node)\n","\n","\n","def construct_tweet_node_from_nx_node(node_id, graph: nx.DiGraph):\n","    return tweet_node(tweet_id=graph.node[node_id]['tweet_id'],\n","                      created_time=graph.node[node_id]['time'],\n","                      node_type=graph.node[node_id]['type'],\n","                      user_id=graph.node[node_id]['user'],\n","                      botometer_score=graph.node[node_id].get('bot_score', None),\n","                      sentiment=graph.node[node_id].get('sentiment', None))\n","\n","\n","def get_dataset_sample_ids(news_source, news_label, dataset_dir=\"data/sample_ids\"):\n","    sample_list = []\n","    with open(\"{}/{}_{}_ids_list.txt\".format(dataset_dir, news_source, news_label)) as file:\n","        for id in file:\n","            sample_list.append(id.strip())\n","\n","    return sample_list\n","\n","\n","def load_from_nx_graphs(dataset_dir: str, news_source: str, news_label: str):\n","    tweet_node_objects = []\n","\n","    news_dataset_dir = \"{}/{}_{}\".format(dataset_dir, news_source, news_label)\n","\n","    for sample_id in get_dataset_sample_ids(news_source, news_label, \"data/sample_ids\"):\n","        with open(\"{}/{}.json\".format(news_dataset_dir, sample_id)) as file:\n","            tweet_node_objects.append(construct_tweet_node_from_json(json.load(file)))\n","\n","    return tweet_node_objects\n","\n","\n","def load_networkx_graphs(dataset_dir: str, news_source: str, news_label: str):\n","    news_dataset_dir = \"{}/{}_{}\".format(dataset_dir, news_source, news_label)\n","    news_samples = []\n","\n","    for news_file in os.listdir(news_dataset_dir):\n","        with open(news_dataset_dir+\"/\"+news_file) as file:\n","            news_samples.append(json_graph.tree_graph(json.load(file)))\n","\n","    return news_samples\n","\n","\n","def load_dataset(dataset_dir: str, news_source: str):\n","    fake_news_samples = load_networkx_graphs(dataset_dir, news_source, \"fake\")\n","    real_news_samples = load_networkx_graphs(dataset_dir, news_source, \"real\")\n","\n","    return fake_news_samples, real_news_samples\n","\n","\n","if __name__ == '__main__':\n","    fake_samples, real_samples = load_dataset(\"data/nx_network_data\", \"politifact\")\n"]},{"cell_type":"code","source":["!pip install vaderSentiment"],"metadata":{"id":"5e9aVmaN8K_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#LINGUISTIC TEST"],"metadata":{"id":"l7bewXbk7w6K"}},{"cell_type":"code","source":["import pickle\n","import queue\n","from pathlib import Path\n","\n","import numpy as np\n","from scipy.spatial.distance import cosine\n","from tqdm import tqdm\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","def get_deepest_cascade_reply_nodes_avg_sentiment(prop_graph: tweet_node):\n","    deep_cascade, max_height = get_post_tweet_deepest_cascade(prop_graph)\n","\n","    return get_reply_nodes_average_sentiment(deep_cascade)\n","\n","\n","def get_deepest_cascade_first_level_reply_sentiment(prop_graph: tweet_node):\n","    deep_cascade, max_height = get_post_tweet_deepest_cascade(prop_graph)\n","    return get_first_reply_nodes_average_sentiment(deep_cascade)\n","\n","\n","def get_first_reply_nodes_average_sentiment(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","    reply_diff_values = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","        for child in node.reply_children:\n","            q.put(child)\n","\n","            if child.node_type == REPLY_NODE and node.node_type == POST_NODE:\n","                if child.sentiment:\n","                    reply_diff_values.append(child.sentiment)\n","\n","    if len(reply_diff_values) == 0:\n","        return 0\n","    else:\n","        return np.mean(np.array(reply_diff_values))\n","\n","\n","def get_reply_nodes_average_sentiment(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","    reply_diff_values = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","        for child in node.reply_children:\n","            q.put(child)\n","\n","        if node.node_type == REPLY_NODE:\n","            if node.sentiment:\n","                reply_diff_values.append(node.sentiment)\n","\n","    if len(reply_diff_values) == 0:\n","        return 0\n","    else:\n","        return np.mean(np.array(reply_diff_values))\n","\n","\n","def get_cosine_similarity(reply_node1, reply_node2, reply_id_index_dict, reply_lat_embeddings):\n","    try:\n","        if reply_node1 in reply_id_index_dict and reply_node2 in reply_id_index_dict:\n","            reply1_idx = reply_id_index_dict[reply_node1]\n","            reply2_idx = reply_id_index_dict[reply_node2]\n","\n","            return cosine(reply_lat_embeddings[reply1_idx], reply_lat_embeddings[reply2_idx])\n","\n","        else:\n","            return 0\n","    except:\n","        return 0\n","\n","\n","def get_supporting_opposing_replies_ratio(prop_graph: tweet_node, news_source, label):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","    similarity_values = list()\n","\n","    reply_id_index_dict = pickle.load(\n","        open(\"data/pre_process_data/elmo_features/{}_{}_reply_id_latent_mat_index.pkl\".format(news_source, label),\n","             \"rb\"))\n","    reply_content_latent_embeddings = pickle.load(\n","        open(\"data/pre_process_data/elmo_features/{}_{}_elmo_lat_embeddings.pkl\".format(news_source, label), \"rb\"))\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","        for child in node.reply_children:\n","            q.put(child)\n","\n","            if node.node_type == REPLY_NODE and child.node_type == REPLY_NODE:\n","                similarity_values.append(get_cosine_similarity(node.tweet_id, child.tweet_id,\n","                                                               reply_id_index_dict, reply_content_latent_embeddings))\n","\n","    if len(similarity_values) == 0:\n","        return 0\n","    else:\n","        supporting = 1\n","        opposing = 1\n","\n","        for value in similarity_values:\n","            if value > 0.5:\n","                supporting += 1\n","            else:\n","                opposing += 1\n","\n","        return supporting / opposing\n","\n","\n","def get_reply_nodes_sentiment_ratio(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","    reply_diff_values = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","        for child in node.reply_children:\n","            q.put(child)\n","\n","        if node.node_type == REPLY_NODE:\n","            reply_diff_values.append(node.sentiment)\n","\n","    if len(reply_diff_values) == 0:\n","        return 0\n","    else:\n","        positive_sentiment = 1\n","        negative_sentiment = 1\n","        for value in reply_diff_values:\n","            if value > 0.05:\n","                positive_sentiment += 1\n","            elif value < -0.05:\n","                negative_sentiment += 1\n","\n","        return positive_sentiment / negative_sentiment\n","\n","\n","def get_stats_for_features(news_graps: list, get_feature_fun_ref, print=False, feature_name=None):\n","    result = []\n","    for graph in news_graps:\n","        result.append(get_feature_fun_ref(graph))\n","\n","    if print:\n","        print_stat_values(feature_name, result)\n","\n","    return result\n","\n","\n","def get_all_linguistic_features(news_graphs, micro_features, macro_features):\n","    all_features = []\n","\n","    if macro_features:\n","        retweet_function_references = []\n","\n","        for function_reference in retweet_function_references:\n","            features_set = get_stats_for_features(news_graphs, function_reference, print=False, feature_name=None)\n","            all_features.append(features_set)\n","\n","    if micro_features:\n","\n","        reply_function_references = [get_reply_nodes_average_sentiment, get_first_reply_nodes_average_sentiment,\n","                                     get_deepest_cascade_reply_nodes_avg_sentiment,\n","                                     get_deepest_cascade_first_level_reply_sentiment]\n","\n","        for function_reference in reply_function_references:\n","            features_set = get_stats_for_features(news_graphs, function_reference, print=True, feature_name=None)\n","            all_features.append(features_set)\n","\n","    return np.transpose(get_numpy_array(all_features))\n","\n","\n","def dump_tweet_reply_sentiment(data_dir, out_dir):\n","    reply_id_content_dict = dict()\n","\n","    reply_id_content_dict.update(pickle.load(\n","        open(\"{}/{}_{}_reply_id_content_dict.pkl\".format(data_dir, \"politifact\", \"fake\"), \"rb\")))\n","\n","    reply_id_content_dict.update(pickle.load(\n","        open(\"{}/{}_{}_reply_id_content_dict.pkl\".format(data_dir, \"politifact\", \"real\"), \"rb\")))\n","\n","    reply_id_content_dict.update(pickle.load(\n","        open(\"{}/{}_{}_reply_id_content_dict.pkl\".format(data_dir, \"gossipcop\", \"fake\"), \"rb\")))\n","\n","    reply_id_content_dict.update(pickle.load(\n","        open(\"{}/{}_{}_reply_id_content_dict.pkl\".format(data_dir, \"gossipcop\", \"real\"), \"rb\")))\n","\n","    print(\"Total no. of replies : {}\".format(len(reply_id_content_dict)))\n","\n","    analyzer = SentimentIntensityAnalyzer()\n","\n","    reply_id_sentiment_output = dict()\n","\n","    for reply_id, content in tqdm(reply_id_content_dict.items()):\n","        sentiment_result = analyzer.polarity_scores(content)\n","        reply_id_sentiment_output[reply_id] = sentiment_result\n","\n","    pickle.dump(reply_id_sentiment_output, open(\"{}/all_reply_id_sentiment_result.pkl\".format(out_dir), \"wb\"))\n","\n","\n","class LinguisticFeatureHelper(BaseFeatureHelper):\n","\n","    def get_feature_group_name(self):\n","        return \"ling\"\n","\n","    def get_micro_feature_method_references(self):\n","        method_refs = [get_reply_nodes_sentiment_ratio,\n","                       get_reply_nodes_average_sentiment,\n","                       get_first_reply_nodes_average_sentiment,\n","                       get_deepest_cascade_reply_nodes_avg_sentiment,\n","                       get_deepest_cascade_first_level_reply_sentiment]\n","\n","        return method_refs\n","\n","    def get_micro_feature_method_names(self):\n","        feature_names = [\"Sentiment ratio of all replies\",\n","                         \"Average sentiment of all replies\",\n","                         \"Average sentiment of first level replies\",\n","                         \"Average sentiment of replies in deepest cascade\",\n","                         \"Average setiment of first level replies in deepest cascade\"]\n","\n","        return feature_names\n","\n","    def get_micro_feature_short_names(self):\n","        feature_names = [\"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\"]\n","        return feature_names\n","\n","    def get_macro_feature_method_references(self):\n","        method_refs = []\n","\n","        return method_refs\n","\n","    def get_macro_feature_method_names(self):\n","        feature_names = []\n","\n","        return feature_names\n","\n","    feature_names = []\n","\n","    def get_macro_feature_short_names(self):\n","        feature_names = []\n","        return feature_names\n","\n","    def get_features_array(self, prop_graphs, micro_features, macro_features, news_source=None, label=None,\n","                           file_dir=\"data/features\", use_cache=False):\n","        function_refs = []\n","\n","        file_name = self.get_dump_file_name(news_source, micro_features, macro_features, label, file_dir)\n","        data_file = Path(file_name)\n","\n","        if use_cache and data_file.is_file():\n","            return pickle.load(open(file_name, \"rb\"))\n","\n","        if micro_features:\n","            function_refs.extend(self.get_micro_feature_method_references())\n","\n","        if len(function_refs) == 0:\n","            return None\n","\n","        all_features = []\n","\n","        for idx in range(len(function_refs)):\n","            features_set = get_sample_feature_value(prop_graphs, function_refs[idx])\n","            all_features.append(features_set)\n","\n","        feature_array = np.transpose(get_numpy_array(all_features))\n","\n","        pickle.dump(feature_array, open(file_name, \"wb\"))\n","\n","        return feature_array\n","\n","\n","def get_feature_involving_additional_args(prop_graphs, function_reference, news_source, label):\n","    feature_values = []\n","    for prop_graph in prop_graphs:\n","        feature_values.append(function_reference(prop_graph, news_source, label))\n","\n","    return feature_values\n"],"metadata":{"id":"dQzwfxYO70JA","executionInfo":{"status":"ok","timestamp":1682317656347,"user_tz":-330,"elapsed":415,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["#TEMPORAL ANALYSIS"],"metadata":{"id":"iL7eKK1L85Y-"}},{"cell_type":"code","source":["import queue\n","\n","import numpy as np\n","\n","\n","def get_avg_retweet_time_deepest_cascade(news_graph: tweet_node):\n","    deep_cascade, max_height = get_post_tweet_deepest_cascade(news_graph)\n","    return get_avg_time_between_retweets(deep_cascade)\n","\n","\n","def get_time_diff_post_time_last_retweet_time_deepest_cascade(news_graph: tweet_node):\n","    deep_cascade, max_height = get_post_tweet_deepest_cascade(news_graph)\n","    first_post_time = deep_cascade.created_time\n","\n","    last_retweet_time = get_last_retweet_by_time(deep_cascade)\n","    return last_retweet_time - first_post_time\n","\n","\n","def get_avg_time_between_replies(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","    reply_diff_values = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        for child in node.children:\n","            q.put(child)\n","\n","            if node.node_type == REPLY_NODE and child.node_type == REPLY_NODE:\n","                reply_diff_values.append(child.created_time - node.created_time)\n","\n","    if len(reply_diff_values) == 0:\n","        return 0\n","    else:\n","        return np.mean(np.array(reply_diff_values))\n","\n","\n","def get_avg_time_between_retweets(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","    retweet_diff_values = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","        for child in node.retweet_children:\n","            q.put(child)\n","            if node.node_type == RETWEET_NODE and child.node_type == RETWEET_NODE:\n","                retweet_diff_values.append(child.created_time - node.created_time)\n","\n","    if len(retweet_diff_values) == 0:\n","        return 0\n","    else:\n","        return np.mean(np.array(retweet_diff_values))\n","\n","\n","def get_last_retweet_by_time(news_graph: tweet_node):\n","    max_time = 0\n","\n","    if news_graph:\n","        for node in news_graph.retweet_children:\n","            max_time = max(max_time, get_last_retweet_by_time(node))\n","\n","    if news_graph and news_graph.created_time is not None:\n","        max_time = max(max_time, news_graph.created_time)\n","\n","    return max_time\n","\n","\n","def get_last_reply_by_time(news_graph: tweet_node):\n","    max_time = 0\n","\n","    if news_graph:\n","        for node in news_graph.retweet_children:\n","            max_time = max(max_time, get_last_retweet_by_time(node))\n","\n","    if news_graph and news_graph.created_time is not None and news_graph.node_type == REPLY_NODE:\n","        max_time = max(max_time, news_graph.created_time)\n","\n","    return max_time\n","\n","\n","def get_avg_time_between_replies_deepest_cascade(news_graph: tweet_node):\n","    deep_cascade, max_height = get_post_tweet_deepest_cascade(news_graph, edge_type=REPLY_EDGE)\n","    return get_avg_time_between_replies(deep_cascade)\n","\n","\n","def get_time_diff_post_time_last_reply_time_deepest_cascade(news_graph: tweet_node):\n","    deep_cascade, max_height = get_post_tweet_deepest_cascade(news_graph, edge_type=REPLY_EDGE)\n","    first_post_time = deep_cascade.created_time\n","\n","    last_reply_time = get_last_reply_by_time(deep_cascade)\n","\n","    if last_reply_time == 0:\n","        return 0\n","    else:\n","        return last_reply_time - first_post_time\n","\n","\n","def get_time_diff_first_post_last_reply(news_graph: tweet_node):\n","    first_post_time = get_first_post_time(news_graph)\n","    last_reply_time = get_last_reply_by_time(news_graph)\n","    if last_reply_time == 0:\n","        return 0\n","    else:\n","        return last_reply_time - first_post_time\n","\n","\n","def get_first_reply_by_time(news_graph: tweet_node):\n","    min_time = float(\"inf\")\n","\n","    if news_graph:\n","        for node in news_graph.retweet_children:\n","            min_time = min(min_time, get_first_retweet_by_time(node))\n","\n","    if news_graph and news_graph.created_time is not None and news_graph.node_type == REPLY_NODE:\n","        min_time = min(min_time, news_graph.created_time)\n","\n","    return min_time\n","\n","\n","def get_first_retweet_by_time(news_graph: tweet_node):\n","    min_time = float(\"inf\")\n","\n","    if news_graph:\n","        for node in news_graph.retweet_children:\n","            min_time = min(min_time, get_first_retweet_by_time(node))\n","\n","    if news_graph and news_graph.created_time is not None and news_graph.node_type == RETWEET_NODE:\n","        min_time = min(min_time, news_graph.created_time)\n","\n","    return min_time\n","\n","\n","def get_time_diff_first_post_last_retweet(news_graph: tweet_node):\n","    first_post_time = get_first_post_time(news_graph)\n","    last_retweet_time = get_last_retweet_by_time(news_graph)\n","    return last_retweet_time - first_post_time\n","\n","\n","def get_time_diff_first_post_first_retweet(news_graph: tweet_node):\n","    first_post_time = get_first_post_time(news_graph)\n","    first_retweet_time = get_first_retweet_by_time(news_graph)\n","\n","    if first_retweet_time == float(\"inf\"):\n","        return 0\n","\n","    return first_retweet_time - first_post_time\n","\n","\n","def get_time_diff_first_last_post_tweet(news_graph: tweet_node):\n","    post_tweets = list(news_graph.children)\n","\n","    if len(post_tweets) <= 1:\n","        # print(\"only one tweet\")\n","        return 0\n","\n","    post_tweets = sort_tweet_node_object_by_created_time(post_tweets)\n","\n","    return post_tweets[len(post_tweets) - 1].created_time - post_tweets[0].created_time\n","\n","\n","def get_average_time_between_post_tweets(news_graph: tweet_node):\n","    post_tweets = list(news_graph.children)\n","\n","    if len(post_tweets) <= 1:\n","        # print(\"only one tweet\")\n","        return 0\n","\n","    post_tweets = sort_tweet_node_object_by_created_time(post_tweets)\n","\n","    time_diff = []\n","\n","    for i in range(1, len(post_tweets)):\n","        time_diff.append(post_tweets[i].created_time - post_tweets[i - 1].created_time)\n","\n","    return np.mean(time_diff)\n","\n","\n","def get_stats_for_features(news_graps: list, get_feature_fun_ref, print=False, feature_name=None):\n","    result = []\n","    for graph in news_graps:\n","        result.append(get_feature_fun_ref(graph))\n","\n","    if print:\n","        print_stat_values(feature_name, result)\n","\n","    return result\n","\n","\n","def print_stat_values(feature_name, values):\n","    print(\"=========================================\")\n","    print(\"Feature : {}\".format(feature_name))\n","    print(\"Min value : {}\".format(min(values)))\n","    print(\"Max value : {}\".format(max(values)))\n","    print(\"Mean value : {}\".format(np.mean(np.array(values))))\n","    print(\"=========================================\")\n","\n","\n","def graph_has_retweet(news_graph: tweet_node):\n","    post_tweets = news_graph.children\n","\n","    for post in post_tweets:\n","        if len(post.retweet_children) > 0:\n","            return True\n","\n","    return False\n","\n","\n","def count_graph_with_no_retweets(news_graphs: list):\n","    count = 0\n","\n","    for prop_graph in news_graphs:\n","        if not graph_has_retweet(prop_graph):\n","            count += 1\n","\n","    print(\"Graph with no retweets : {}\".format(count))\n","\n","\n","def get_all_temporal_features(prop_graphs, micro_features, macro_features):\n","    macro_features_functions = [get_average_time_between_post_tweets,\n","                                get_time_diff_first_last_post_tweet,\n","                                get_time_diff_first_post_last_retweet,\n","                                get_time_diff_first_post_first_retweet,\n","                                get_avg_time_between_retweets,\n","                                get_avg_retweet_time_deepest_cascade,\n","                                get_time_diff_post_time_last_retweet_time_deepest_cascade]\n","\n","    micro_features_functions = [get_avg_time_between_replies,\n","                                get_time_diff_first_post_last_reply,\n","                                get_time_diff_post_time_last_reply_time_deepest_cascade]\n","\n","    function_refs = []\n","\n","    if macro_features:\n","        function_refs.extend(macro_features_functions)\n","\n","    if micro_features:\n","        function_refs.extend(micro_features_functions)\n","\n","    all_features = []\n","\n","    for function_reference in function_refs:\n","        features_set = get_stats_for_features(prop_graphs, function_reference, print=False, feature_name=None)\n","        all_features.append(features_set)\n","\n","    return np.transpose(get_numpy_array(all_features))\n","\n","\n","def time_difference_between_first_post_node_with_max_out_degree_macro(prop_graph):\n","    max_out_degree_node, max_out_degree = get_max_out_degree_node(prop_graph, RETWEET_EDGE)\n","    first_post_time = get_first_post_time(prop_graph)\n","    if max_out_degree_node is None:\n","        return 0\n","    return max_out_degree_node.created_time - first_post_time\n","\n","\n","def get_time_diff_first_post_first_reply(news_graph):\n","    first_reply_time = get_first_reply_by_time(news_graph)\n","    first_post_time = get_first_post_time(news_graph)\n","\n","    if first_reply_time == float(\"inf\"):\n","        return 0\n","\n","    return first_reply_time - first_post_time\n","\n","\n","class TemporalFeatureHelper(BaseFeatureHelper):\n","\n","    def get_feature_group_name(self):\n","        return \"temp\"\n","\n","    def get_micro_feature_method_references(self):\n","        method_refs = [get_avg_time_between_replies,\n","                       get_time_diff_first_post_first_reply,\n","                       get_time_diff_first_post_last_reply,\n","                       get_avg_time_between_replies_deepest_cascade,\n","                       get_time_diff_post_time_last_reply_time_deepest_cascade]\n","\n","        return method_refs\n","\n","    def get_micro_feature_method_names(self):\n","        feature_names = [\"Average time diff between adjacent replies\",\n","                         \"Time diff between first tweet posting node and first reply node\",\n","                         \"Time diff between first tweet posting news and last reply node\",\n","                         \"Average time between adjacent reply nodes in the deepest cascade\",\n","                         \"Time diff between first tweet posting news and last reply node in the deepest cascade\"]\n","\n","        return feature_names\n","\n","    def get_micro_feature_short_names(self):\n","        feature_names = [\"T9\", \"T10\", \"T11\", \"T12\", \"T13\"]\n","        return feature_names\n","\n","    def get_macro_feature_method_references(self):\n","        method_refs = [get_avg_time_between_retweets,\n","                       get_time_diff_first_post_last_retweet,\n","                       time_difference_between_first_post_node_with_max_out_degree_macro,  # not implemented\n","                       get_time_diff_first_last_post_tweet,\n","                       get_time_diff_post_time_last_retweet_time_deepest_cascade,\n","                       get_avg_retweet_time_deepest_cascade,\n","                       get_average_time_between_post_tweets,\n","                       get_time_diff_first_post_first_retweet]\n","\n","        return method_refs\n","\n","    def get_macro_feature_method_names(self):\n","        feature_names = [\"Average time diff between the adjacent retweet nodes in macro network\",\n","                         \"Time diff between first tweet and  most recent node in macro network\",\n","                         \"Time diff between first tweet posting news and max out degree node\",\n","                         \"Time difference between the first and last tweet posting news\",\n","                         \"Time diff between tweet posting news and latest retweet node in the deepest cascade\",\n","                         \"Average time diff between the adjacent retweet nodes in deepest cascade\",\n","                         \"Average time between the tweets posted related to news\",\n","                         \"Avg time diff between the tweet post time and the first retweet time\"]\n","\n","        return feature_names\n","\n","    def get_macro_feature_short_names(self):\n","        feature_names = [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\", \"T6\", \"T7\", \"T8\"]\n","        return feature_names\n"],"metadata":{"id":"x81KGc3388I7","executionInfo":{"status":"ok","timestamp":1682317788138,"user_tz":-330,"elapsed":1,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["#STRUCTURAL ANALYSIS"],"metadata":{"id":"jWh_MXKH8qde"}},{"cell_type":"code","source":["import pickle\n","import queue\n","import time\n","from pathlib import Path\n","\n","import numpy as np\n","\n","\n","def get_post_tweet_deepest_cascade(prop_graph: tweet_node, edge_type=RETWEET_EDGE):\n","    max_height = 0\n","    max_height_node = None\n","\n","    for node in prop_graph.children:\n","        height = get_tree_height(node, edge_type)\n","        if height > max_height:\n","            max_height = height\n","            max_height_node = node\n","\n","    return max_height_node, max_height\n","\n","\n","def get_num_cascade(node: tweet_node, edge_type=\"retweet\"):\n","    if edge_type == \"retweet\":\n","        return len(node.retweet_children)\n","    elif edge_type == \"reply\":\n","        return len(node.reply_children)\n","    else:\n","        return len(node.children)\n","\n","\n","def get_temp_num_cascade(node: tweet_node, edge_type=\"retweet\", max_time=time.time()):\n","    if edge_type == \"retweet\":\n","        children = node.retweet_children\n","    elif edge_type == \"reply\":\n","        children = node.reply_children\n","    else:\n","        children = node.children\n","\n","    cascade_count = 0\n","\n","    for child in children:\n","        if child.created_time <= max_time:\n","            cascade_count += 1\n","\n","    return cascade_count\n","\n","\n","def get_node_count_deepest_cascade(news_graphs: tweet_node, edge_type):\n","    node_counts = []\n","\n","    for prop_graph in news_graphs:\n","        max_height_node, max_height = get_post_tweet_deepest_cascade(prop_graph)\n","\n","        node_counts.append(get_nodes_count(max_height_node, edge_type))\n","\n","    return node_counts\n","\n","\n","def get_max_outdegree(node: tweet_node, edge_type=\"retweet\"):\n","    if node is None:\n","        return 0\n","\n","    if edge_type == \"retweet\":\n","        children = node.retweet_children\n","    elif edge_type == \"reply\":\n","        children = node.reply_children\n","    else:\n","        children = node.children\n","\n","    if node.node_type == NEWS_ROOT_NODE:\n","        max_outdegree = 0\n","    else:\n","        max_outdegree = len(children)\n","\n","    for child in children:\n","        max_outdegree = max(max_outdegree, get_max_outdegree(child, edge_type))\n","\n","    return max_outdegree\n","\n","\n","def get_max_out_degree_node(node: tweet_node, edge_type=RETWEET_EDGE):\n","    if node is None:\n","        return None\n","\n","    if edge_type == \"retweet\":\n","        children = node.retweet_children\n","    elif edge_type == \"reply\":\n","        children = node.reply_children\n","    else:\n","        children = node.children\n","\n","    if node.node_type == NEWS_ROOT_NODE:\n","        max_outdegree_node, max_out_degree = None, 0\n","\n","    else:\n","        max_outdegree_node, max_out_degree = node, len(children)\n","\n","    for child in children:\n","        child_max_out_degree_node, child_max_out_degree = get_max_out_degree_node(child, edge_type)\n","        if child_max_out_degree > max_out_degree:\n","            max_out_degree = child_max_out_degree\n","            max_outdegree_node = child_max_out_degree_node\n","\n","    return max_outdegree_node, max_out_degree\n","\n","\n","def get_target_node_level(root_node: tweet_node, target_node, level=0):\n","    if root_node is None:\n","        return 0\n","\n","    if root_node.tweet_id == target_node.tweet_id:\n","        return level\n","\n","    for child in root_node.children:\n","        res_level = get_target_node_level(child, target_node, level + 1)\n","        if res_level != 0:\n","            return res_level\n","\n","    return 0\n","\n","\n","def get_depth_of_max_degree_node(prop_graph, edge_type=RETWEET_EDGE):\n","    max_out_degree_node, max_out_degree = get_max_out_degree_node(prop_graph, edge_type)\n","\n","    if max_out_degree_node is None:\n","        return 0\n","\n","    return get_target_node_level(prop_graph, max_out_degree_node, 0)\n","\n","\n","def get_max_out_degree_depths(prop_graphs, edge_type=RETWEET_EDGE):\n","    out_degree_depths = []\n","\n","    for news_node in prop_graphs:\n","        out_degree_depths.append(get_depth_of_max_degree_node(news_node, edge_type))\n","\n","    return out_degree_depths\n","\n","\n","def get_tree_height(node, edge_type=\"retweet\"):\n","    if node is None:\n","        return 0\n","\n","    max_child_height = 0\n","\n","    if edge_type == \"retweet\":\n","        children = node.retweet_children\n","    elif edge_type == \"reply\":\n","        children = node.reply_children\n","    else:\n","        children = node.children\n","\n","    for child in children:\n","        max_child_height = max(max_child_height, get_tree_height(child, edge_type))\n","\n","    return max_child_height + 1\n","\n","\n","def get_nodes_count(node: tweet_node, edge_type=\"retweet\"):\n","    if node is None:\n","        return 0\n","\n","    node_count = 0\n","\n","    if edge_type == \"retweet\":\n","        children = node.retweet_children\n","    elif edge_type == \"reply\":\n","        children = node.reply_children\n","    else:\n","        children = node.children\n","\n","    for child in children:\n","        node_count += get_nodes_count(child, edge_type)\n","\n","    return node_count + 1\n","\n","\n","def get_temporal_nodes_count(node: tweet_node, edge_type=\"retweet\", max_time=time.time()):\n","    if node is None or (node.created_time is not None and node.created_time > max_time):\n","        return 0\n","\n","    node_count = 0\n","\n","    if edge_type == \"retweet\":\n","        children = node.retweet_children\n","    elif edge_type == \"reply\":\n","        children = node.reply_children\n","    else:\n","        children = node.children\n","\n","    for child in children:\n","        node_count += get_temporal_nodes_count(child, edge_type, max_time)\n","\n","    return node_count + 1\n","\n","\n","def get_node_size_by_time(prop_graphs: list, edge_type: str, time_interval_sec: list):\n","    temporal_tree_node_size = []\n","    for news_node in prop_graphs:\n","        temp_node_sizes = []\n","        first_post_time = get_first_post_time(news_node)\n","        for time_limit in time_interval_sec:\n","            node_count = get_temporal_nodes_count(news_node, edge_type, first_post_time + time_limit)\n","            temp_node_sizes.append(node_count)\n","\n","        temporal_tree_node_size.append(temp_node_sizes)\n","\n","    return temporal_tree_node_size\n","\n","\n","def get_temporal_tree_height(node: tweet_node, edge_type=\"retweet\", max_time=time.time()):\n","    if node is None or (node.created_time is not None and node.created_time > max_time):\n","        return 0\n","\n","    max_child_height = 0\n","\n","    if edge_type == \"retweet\":\n","        children = node.retweet_children\n","    elif edge_type == \"reply\":\n","        children = node.reply_children\n","    else:\n","        children = node.children\n","\n","    for child in children:\n","        max_child_height = max(max_child_height, get_temporal_tree_height(child, edge_type, max_time))\n","\n","    return max_child_height + 1\n","\n","\n","def get_num_cascades_by_time(prop_graphs: list, edge_type: str, time_interval_sec: list):\n","    temporal_num_cascades = []\n","    for news_node in prop_graphs:\n","        temp_cascade_num = []\n","        first_post_time = get_first_post_time(news_node)\n","        for time_limit in time_interval_sec:\n","            node_count = get_temp_num_cascade(news_node, edge_type, first_post_time + time_limit)\n","            temp_cascade_num.append(node_count)\n","\n","        temporal_num_cascades.append(temp_cascade_num)\n","\n","    return temporal_num_cascades\n","\n","\n","def get_tree_heights(news_graphs: list, edge_type):\n","    heights = []\n","\n","    for news_node in news_graphs:\n","        heights.append(get_tree_height(news_node, edge_type))\n","\n","    return heights\n","\n","\n","def analyze_height(news_graphs: list, edge_type):\n","    heights = get_tree_heights(news_graphs, edge_type)\n","\n","    print(\"----HEIGHT-----\")\n","\n","    print(\"max\", max(heights))\n","    print(\"min\", min(heights))\n","    print(\"avg\", np.mean(heights))\n","\n","\n","def get_max_outdegrees(news_graphs: list, edge_type):\n","    max_outdegrees = []\n","\n","    for news_node in news_graphs:\n","        max_outdegrees.append(get_max_outdegree(news_node, edge_type))\n","\n","    return max_outdegrees\n","\n","\n","def analyze_max_outdegree(news_graphs: list, edge_type):\n","    max_outdegrees = get_max_outdegrees(news_graphs, edge_type)\n","    print(\"-----MAX - OUT DEGREE -----\")\n","    print(\"max\", max(max_outdegrees))\n","    print(\"min\", min(max_outdegrees))\n","    print(\"avg\", np.mean(max_outdegrees))\n","\n","\n","def get_prop_graps_cascade_num(news_graphs: list, edge_type):\n","    cascade_num = []\n","\n","    for news_node in news_graphs:\n","        cascade_num.append(get_num_cascade(news_node, edge_type))\n","\n","    return cascade_num\n","\n","\n","def analyze_cascade(news_graphs: list, edge_type):\n","    cascade_num = get_prop_graps_cascade_num(news_graphs, edge_type)\n","\n","    print(\"-----CASCADE-----\")\n","    print(\"max\", max(cascade_num))\n","    print(\"min\", min(cascade_num))\n","    print(\"avg\", np.mean(cascade_num))\n","\n","\n","def get_prop_graphs_node_counts(news_graphs: list, edge_type):\n","    node_counts = []\n","\n","    for news_node in news_graphs:\n","        node_counts.append(get_nodes_count(news_node, edge_type))\n","\n","    return node_counts\n","\n","\n","def analyze_node_count(news_graphs: list, edge_type):\n","    node_counts = get_prop_graphs_node_counts(news_graphs, edge_type)\n","\n","    print(\"----NODE SIZE-----\")\n","\n","    print(\"max\", max(node_counts))\n","    print(\"min\", min(node_counts))\n","    print(\"avg\", np.mean(node_counts))\n","\n","\n","def get_height_by_time(prop_graphs: list, edge_type: str, time_interval_sec: list):\n","    temporal_tree_height = []\n","    for news_node in prop_graphs:\n","        temp_heights = []\n","        first_post_time = get_first_post_time(news_node)\n","        for time_limit in time_interval_sec:\n","            height = get_temporal_tree_height(news_node, edge_type, first_post_time + time_limit)\n","            temp_heights.append(height)\n","\n","        temporal_tree_height.append(temp_heights)\n","\n","    return temporal_tree_height\n","\n","\n","def analyze_height_by_time(prop_graphs: list, edge_type: str, time_interval_sec: list):\n","    temporal_tree_height = get_height_by_time(prop_graphs, edge_type, time_interval_sec)\n","\n","    temporal_tree_height = np.array([np.array(val) for val in temporal_tree_height])\n","\n","    for idx, time_limit_sec in enumerate(time_interval_sec):\n","        heights_at_time = temporal_tree_height[:, idx]\n","        print(\"Time limit: {}\".format(time_limit_sec))\n","        print(\"Min height : {}\".format(np.min(heights_at_time)))\n","        print(\"Max height : {}\".format(np.max(heights_at_time)))\n","        print(\"Mean height : {}\".format(np.mean(heights_at_time)))\n","        print(flush=True)\n","\n","\n","def analyze_cascade_num_by_time(prop_graphs: list, edge_type: str, time_interval_sec: list):\n","    temporal_cascade_num = get_num_cascades_by_time(prop_graphs, edge_type, time_interval_sec)\n","\n","    temporal_cascade_num = np.array([np.array(val) for val in temporal_cascade_num])\n","\n","    for idx, time_limit_sec in enumerate(time_interval_sec):\n","        heights_at_time = temporal_cascade_num[:, idx]\n","        print(\"Time limit: {}\".format(time_limit_sec))\n","        print(\"Min num cascade : {}\".format(np.min(heights_at_time)))\n","        print(\"Max num cascade : {}\".format(np.max(heights_at_time)))\n","        print(\"Mean num cascade : {}\".format(np.mean(heights_at_time)))\n","        print(flush=True)\n","\n","\n","def analyze_node_size_by_time(prop_graphs: list, edge_type: str, time_interval_sec: list):\n","    temporal_tree_node_sizes = get_node_size_by_time(prop_graphs, edge_type, time_interval_sec)\n","\n","    temporal_tree_node_sizes = np.array([np.array(val) for val in temporal_tree_node_sizes])\n","\n","    for idx, time_limit_sec in enumerate(time_interval_sec):\n","        heights_at_time = temporal_tree_node_sizes[:, idx]\n","        print(\"Time limit: {}\".format(time_limit_sec))\n","        print(\"Min node size : {}\".format(np.min(heights_at_time)))\n","        print(\"Max node size : {}\".format(np.max(heights_at_time)))\n","        print(\"Mean node size : {}\".format(np.mean(heights_at_time)))\n","        print(flush=True)\n","\n","\n","def get_first_post_time(node: tweet_node):\n","    first_post_time = time.time()\n","\n","    for child in node.children:\n","        first_post_time = min(first_post_time, child.created_time)\n","\n","    return first_post_time\n","\n","\n","def get_num_of_cascades_with_retweets(root_node: tweet_node):\n","    num_cascades = 0\n","    for node in root_node.retweet_children:\n","        if len(node.retweet_children) > 0:\n","            num_cascades += 1\n","\n","    return num_cascades\n","\n","\n","def get_prop_graphs_num_of_cascades_with_retweets(prop_graphs, edge_type=RETWEET_EDGE):\n","    return get_sample_feature_value(prop_graphs, get_num_of_cascades_with_retweets)\n","\n","\n","def get_fraction_of_cascades_with_retweets(root_node: tweet_node):\n","    total_cascades = len(root_node.retweet_children)\n","\n","    cascade_with_retweet = 0\n","    for node in root_node.retweet_children:\n","        if len(node.retweet_children) > 0:\n","            cascade_with_retweet += 1\n","\n","    return cascade_with_retweet / total_cascades\n","\n","\n","def get_prop_graphs_fraction_of_cascades_with_retweets(prop_graphs, edge_type=RETWEET_EDGE):\n","    return get_sample_feature_value(prop_graphs, get_fraction_of_cascades_with_retweets)\n","\n","\n","def get_num_of_cascades_with_replies(root_node: tweet_node):\n","    num_cascades = 0\n","    for node in root_node.reply_children:\n","        if len(node.reply_children) > 0:\n","            num_cascades += 1\n","\n","    return num_cascades\n","\n","\n","def get_prop_graphs_num_of_cascades_with_replies(prop_graphs, edge_type=RETWEET_EDGE):\n","    return get_sample_feature_value(prop_graphs, get_num_of_cascades_with_replies)\n","\n","\n","def get_fraction_of_cascades_with_replies(root_node: tweet_node):\n","    total_cascades = len(root_node.reply_children)\n","\n","    cascade_with_replies = 0\n","    for node in root_node.reply_children:\n","        if len(node.reply_children) > 0:\n","            cascade_with_replies += 1\n","\n","    return cascade_with_replies / total_cascades\n","\n","\n","def get_users_in_network(prop_graph: tweet_node, edge_type=None):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","\n","    users_list = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        if edge_type == RETWEET_EDGE:\n","            children = node.retweet_children\n","        elif edge_type == REPLY_EDGE:\n","            children = node.reply_children\n","        else:\n","            children = node.children\n","\n","        for child in children:\n","            q.put(child)\n","            if child.user_id is not None:\n","                users_list.append(child.user_id)\n","\n","    return users_list\n","\n","\n","def get_users_replying_in_prop_graph(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","\n","    users_list = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        for child in node.reply_children:\n","            q.put(child)\n","            if child.node_type == REPLY_NODE and child.user_id is not None:\n","                users_list.append(child.user_id)\n","\n","    return users_list\n","\n","\n","def get_users_retweeting_in_prop_graph(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","\n","    users_list = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        for child in node.retweet_children:\n","            q.put(child)\n","            if child.node_type == RETWEET_NODE and child.user_id is not None:\n","                users_list.append(child.user_id)\n","\n","    return users_list\n","\n","\n","def get_user_names_retweeting_in_prop_graph(prop_graph: tweet_node):\n","    q = queue.Queue()\n","\n","    q.put(prop_graph)\n","\n","    users_list = list()\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        for child in node.retweet_children:\n","            q.put(child)\n","            if child.node_type == RETWEET_NODE and child.user_name is not None:\n","                users_list.append(child.user_name)\n","\n","    return users_list\n","\n","\n","def get_num_user_retweet_and_reply(prop_graph: tweet_node):\n","    retweet_users = set(get_users_retweeting_in_prop_graph(prop_graph))\n","    replying_users = set(get_users_replying_in_prop_graph(prop_graph))\n","\n","    return len(retweet_users.intersection(replying_users))\n","\n","\n","def get_ratio_of_retweet_to_reply(prop_graph: tweet_node):\n","    retweet_users = set(get_users_retweeting_in_prop_graph(prop_graph))\n","    replying_users = set(get_users_replying_in_prop_graph(prop_graph))\n","\n","    return (len(retweet_users) + 1) / (len(replying_users) + 1)\n","\n","\n","def get_prop_graphs_num_user_retweet_and_reply(prop_graphs, edge_type=None):\n","    return get_sample_feature_value(prop_graphs, get_num_user_retweet_and_reply)\n","\n","\n","def get_prop_graphs_ratio_of_retweet_to_reply(prop_graphs, edge_type=None):\n","    return get_sample_feature_value(prop_graphs, get_ratio_of_retweet_to_reply)\n","\n","\n","def get_unique_users_in_graph(prop_graph: tweet_node, edge_type=None):\n","    user_list = get_users_in_network(prop_graph, edge_type)\n","    return len(set(user_list))\n","\n","\n","def get_fraction_of_unique_users(prop_graph: tweet_node, edge_type=None):\n","    user_list = get_users_in_network(prop_graph, edge_type)\n","    try:\n","        return len(set(user_list)) / len(user_list)\n","    except:\n","        print(\"Exception in fraction of unique users\")\n","        return 0\n","\n","\n","def get_num_bot_users(prop_graph: tweet_node):\n","    q = queue.Queue()\n","    q.put(prop_graph)\n","\n","    num_bot_users = 0\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        for child in node.retweet_children:\n","            q.put(child)\n","            if child.node_type == RETWEET_NODE and child.user_id is not None:\n","                if child.botometer_score and child.botometer_score > 0.5:\n","                    num_bot_users += 1\n","\n","    return num_bot_users\n","\n","\n","def get_fraction_of_bot_users_retweeting(prop_graph: tweet_node):\n","    q = queue.Queue()\n","    q.put(prop_graph)\n","\n","    num_bot_users = 1\n","    num_human_users = 1\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        for child in node.retweet_children:\n","            q.put(child)\n","            if child.node_type == RETWEET_NODE and child.user_id is not None:\n","                if child.botometer_score:\n","                    if child.botometer_score > 0.5:\n","                        num_bot_users += 1\n","                    else:\n","                        num_human_users += 1\n","\n","    return num_bot_users / (num_human_users + num_bot_users)\n","\n","\n","def get_prop_graphs_num_bot_users_retweeting(prop_graphs: tweet_node, edge_type=None):\n","    global user_id_bot_score_dict\n","    return get_sample_feature_value(prop_graphs, get_num_bot_users)\n","\n","\n","def get_prop_graphs_fraction_of_bot_users_retweeting(prop_graphs: tweet_node, edge_type=None):\n","    return get_sample_feature_value(prop_graphs, get_fraction_of_bot_users_retweeting)\n","\n","\n","def get_breadth_at_each_level(prop_graph, edge_type=RETWEET_EDGE):\n","    q1 = queue.Queue()\n","    q2 = queue.Queue()\n","\n","    q1.put(prop_graph)\n","\n","    level_breadths = []\n","\n","    while q1.qsize() != 0 or q2.qsize() != 0:\n","\n","        if q1.qsize() != 0:\n","            level_breadths.append(q1.qsize())\n","\n","        while q1.qsize() != 0:\n","            node = q1.get()\n","\n","            if edge_type == RETWEET_EDGE:\n","                children = node.retweet_children\n","            elif edge_type == REPLY_EDGE:\n","                children = node.reply_children\n","            else:\n","                children = node.children\n","\n","            for child in children:\n","                q2.put(child)\n","\n","        if q2.qsize() != 0:\n","            level_breadths.append(q2.qsize())\n","\n","        while q2.qsize() != 0:\n","            node = q2.get()\n","\n","            if edge_type == RETWEET_EDGE:\n","                children = node.retweet_children\n","            elif edge_type == REPLY_EDGE:\n","                children = node.reply_children\n","            else:\n","                children = node.children\n","\n","            for child in children:\n","                q1.put(child)\n","\n","    return max(level_breadths)\n","\n","\n","def get_prop_graphs_max_breadth(prop_graphs, edge_type=RETWEET_EDGE):\n","    return get_sample_feature_value(prop_graphs, get_breadth_at_each_level)\n","\n","\n","def get_prop_graphs_num_unique_users(prop_graphs, edge_type=RETWEET_EDGE):\n","    unique_users_cnts = []\n","\n","    for graph in prop_graphs:\n","        unique_users_cnts.append(get_unique_users_in_graph(graph, edge_type))\n","\n","    return unique_users_cnts\n","\n","\n","def get_prop_graphs_fraction_of_unique_users(prop_graphs, edge_type=RETWEET_EDGE):\n","    unique_users_fract_cnts = []\n","\n","    for graph in prop_graphs:\n","        unique_users_fract_cnts.append(get_fraction_of_unique_users(graph, edge_type))\n","\n","    return unique_users_fract_cnts\n","\n","\n","def get_prop_graphs_fraction_of_cascades_with_replies(prop_graphs, edge_type=RETWEET_EDGE):\n","    return get_sample_feature_value(prop_graphs, get_fraction_of_cascades_with_replies)\n","\n","\n","def get_prop_graphs_min_time_to_reach_level_1(news_graphs: list, edge_type=None):\n","    return get_sample_feature_value(news_graphs, get_min_time_to_reach_level_1)\n","\n","\n","def get_prop_graphs_min_time_to_reach_level_2(news_graphs: list, edge_type=None):\n","    return get_sample_feature_value(news_graphs, get_min_time_to_reach_level_2)\n","\n","\n","def get_min_time_to_reach_level_1(new_graph: tweet_node):\n","    return get_min_time_to_reach_level(new_graph, 1)\n","\n","\n","def get_min_time_to_reach_level_2(news_graph: tweet_node):\n","    return get_min_time_to_reach_level(news_graph, 2)\n","\n","\n","def get_min_time_to_reach_level(new_graph: tweet_node, target_depth):\n","    time_to_reach_depth = []\n","    for post_node in new_graph.retweet_children:\n","        post_time = post_node.created_time\n","        level_node_times = dfs_traverse(post_node, 0, target_depth)\n","        if len(level_node_times) > 0:\n","            time_to_reach_depth.append(min(level_node_times) - post_time)\n","\n","    if len(time_to_reach_depth) > 0:\n","        return np.mean(time_to_reach_depth)\n","    else:\n","        return 0\n","\n","\n","def get_unique_users_untill_level(new_graph: tweet_node, target_depth):\n","    dfs_traverse_get_users(new_graph, target_depth)\n","\n","\n","def dfs_traverse(node: tweet_node, level: int, target: int):\n","    result = []\n","\n","    if level == target:\n","        return [node.created_time]\n","\n","    elif level > target:\n","        return None\n","\n","    else:\n","        for child in node.retweet_children:\n","            level_nodes = dfs_traverse(child, level + 1, target)\n","            if level_nodes:\n","                result.extend(level_nodes)\n","\n","    return result\n","\n","\n","def get_num_unique_users_under_level_2(node: tweet_node, edge_type=None):\n","    return len(dfs_traverse_get_users(node, 0, 2))\n","\n","\n","def get_num_unique_users_under_level_4(node: tweet_node, edge_type=None):\n","    return len(dfs_traverse_get_users(node, 0, 4))\n","\n","\n","def get_prop_graphs_num_unique_user_under_level_2(prop_graphs, edge_type=RETWEET_EDGE):\n","    return get_sample_feature_value(prop_graphs, get_num_unique_users_under_level_2)\n","\n","\n","def get_prop_graphs_num_unique_user_under_level_4(prop_graphs, edge_type=RETWEET_EDGE):\n","    return get_sample_feature_value(prop_graphs, get_num_unique_users_under_level_4)\n","\n","\n","def dfs_traverse_get_users(node: tweet_node, level: int, target: int):\n","    result = list()\n","\n","    if level > target:\n","        return None\n","\n","    else:\n","        result.append(node.user_id)\n","\n","        for child in node.retweet_children:\n","            level_nodes = dfs_traverse(child, level + 1, target)\n","            if level_nodes:\n","                result.extend(level_nodes)\n","\n","    return result\n","\n","\n","def get_all_structural_features(news_graphs, micro_features, macro_features):\n","    all_features = []\n","    target_edge_type = RETWEET_EDGE\n","\n","    if macro_features:\n","        retweet_function_references = [get_tree_heights, get_prop_graphs_node_counts, get_prop_graps_cascade_num,\n","                                       get_max_outdegrees, get_num_of_cascades_with_retweets,\n","                                       get_fraction_of_cascades_with_retweets]\n","        for function_ref in retweet_function_references:\n","            features = function_ref(news_graphs, target_edge_type)\n","            all_features.append(features)\n","\n","    if micro_features:\n","        target_edge_type = REPLY_EDGE\n","\n","        reply_function_references = [get_tree_heights, get_prop_graphs_node_counts, get_max_outdegrees]\n","        for function_ref in reply_function_references:\n","            features = function_ref(news_graphs, target_edge_type)\n","            all_features.append(features)\n","\n","    return np.transpose(get_numpy_array(all_features))\n","\n","\n","class StructureFeatureHelper(BaseFeatureHelper):\n","\n","    def get_feature_group_name(self):\n","        return \"struct\"\n","\n","    def get_micro_feature_method_references(self):\n","        method_refs = [get_tree_heights, get_prop_graphs_node_counts, get_max_outdegrees,\n","                       get_prop_graphs_num_of_cascades_with_replies,\n","                       get_prop_graphs_fraction_of_cascades_with_replies]\n","\n","        return method_refs\n","\n","    def get_micro_feature_method_names(self):\n","        feature_names = [\"Micro - Tree depth\", \"Micro - No of nodes\", \"Micro - Maximum out degree\",\n","                         \"No. of cascades with replies\", \"Fraction of cascades with replies\"]\n","        return feature_names\n","\n","    def get_micro_feature_short_names(self):\n","        feature_names = [\"S10\", \"S11\", \"S12\", \"S13\", \"S14\"]\n","        return feature_names\n","\n","    def get_macro_feature_method_references(self):\n","        method_refs = [get_tree_heights, get_prop_graphs_node_counts, get_max_outdegrees, get_prop_graps_cascade_num,\n","                       get_max_out_degree_depths,\n","                       get_prop_graphs_num_of_cascades_with_retweets,\n","                       get_prop_graphs_fraction_of_cascades_with_retweets,\n","                       get_prop_graphs_num_bot_users_retweeting,\n","                       get_prop_graphs_fraction_of_bot_users_retweeting,\n","                       ]\n","\n","        return method_refs\n","\n","    def get_macro_feature_method_names(self):\n","        feature_names = [\"Macro - Tree depth\",\n","                         \"Macro - No of nodes\",\n","                         \"Macro - Maximum out degree\",\n","                         \"Macro - No of cascades\",\n","                         \"Macro - Max out degree node's level\",\n","                         \"No. of cascades with retweets\",\n","                         \"Fraction of cascades with retweets\",\n","                         \"No. of bot users retweeting\",\n","                         \"Fraction of bot user retweeting\"]\n","\n","        return feature_names\n","\n","    feature_names = []\n","\n","    def get_macro_feature_short_names(self):\n","        feature_names = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\"]\n","        return feature_names\n","\n","    def get_features_array(self, prop_graphs, micro_features, macro_features, news_source=None, label=None,\n","                           file_dir=\"data/features\", use_cache=False):\n","        all_features = []\n","\n","        file_name = self.get_dump_file_name(news_source, micro_features, macro_features, label, file_dir)\n","        data_file = Path(file_name)\n","\n","        if use_cache and data_file.is_file():\n","            return pickle.load(open(file_name, \"rb\"))\n","\n","        if micro_features:\n","            target_edge_type = REPLY_EDGE\n","\n","            reply_function_references = self.get_micro_feature_method_references()\n","            for function_ref in reply_function_references:\n","                features = function_ref(prop_graphs, target_edge_type)\n","                all_features.append(features)\n","\n","        if macro_features:\n","            target_edge_type = RETWEET_EDGE\n","            retweet_function_references = self.get_macro_feature_method_references()\n","            for function_ref in retweet_function_references:\n","                features = function_ref(prop_graphs, target_edge_type)\n","                all_features.append(features)\n","\n","        feature_array = np.transpose(get_numpy_array(all_features))\n","\n","        pickle.dump(feature_array, open(file_name, \"wb\"))\n","\n","        return feature_array\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"63lMwSLY8s95","executionInfo":{"status":"error","timestamp":1682317713070,"user_tz":-330,"elapsed":466,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}},"outputId":"373e36c9-2fbc-4f9a-91da-4d3152cc9eb6"},"execution_count":21,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-91c84830bc3e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_post_tweet_deepest_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_graph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtweet_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRETWEET_EDGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmax_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmax_height_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'RETWEET_EDGE' is not defined"]}]},{"cell_type":"markdown","source":["#STAT TEST"],"metadata":{"id":"SToSKRey79Nc"}},{"cell_type":"code","source":["import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","\n","\n","def perform_t_test(samples1, samples2):\n","    [t_val, p_val] = stats.ttest_ind(samples1, samples2, equal_var=True)\n","    print(\"t-Statistic value : {}\".format(t_val))\n","    print(\"p - value : {}\".format(p_val))\n","    print(\"=====================================\")\n","\n","\n","def plot_normal_distributions(samples1, samples2):\n","    fit1 = stats.norm.pdf(samples1, np.mean(samples1), np.std(samples1))\n","    fit2 = stats.norm.cdf(samples2, np.mean(samples2), np.std(samples2))\n","\n","    plt.plot(sorted(samples1), fit1, 'red')\n","    plt.plot(sorted(samples2), fit2, 'blue')\n","    plt.show()\n","\n","\n","def get_box_plots(samples1, samples2, save_folder, title=None, file_name=None):\n","    all_data = [samples1, samples2]\n","    labels = ['Fake', 'Real']\n","    # plt.box(None)\n","\n","    font = {'family': 'normal',\n","            'weight': 'semibold',\n","            'size': 13}\n","    #\n","    matplotlib.rc('font', **font)\n","\n","    # plt.xlabel('l', fontsize=18)\n","    # plt.ylabel('ylabel', fontsize=16)\n","    plt.tight_layout()\n","    plt.figure(figsize=(1.5, 4))\n","\n","    fig = plt.figure(1, figsize=(1, 3), frameon=False)\n","\n","    ax1 = fig.add_subplot(111)\n","    bplot1 = ax1.boxplot(all_data,\n","                         vert=True,  # vertical box alignment\n","                         patch_artist=True,  # fill with color\n","                         labels=labels,  # will be used to label x-ticks\n","                         showfliers=False,\n","                         positions=[0, 0.5])\n","\n","\n","    # plt.title(title)\n","    # title = ax1.set_title(\"\\n\".join(wrap(title,50)), fontdict={'fontweight': 'semibold'})\n","    [t_val, p_val] = stats.ttest_ind(samples1, samples2, equal_var=True)\n","\n","    if p_val > 0.05:\n","        title = ax1.set_title(file_name, fontdict={'fontweight': 'bold', 'fontsize': 16})\n","    else:\n","        ax1.set_title(r'' + file_name + ' $\\mathbf{^{*}}$', fontdict={'fontweight': 'bold', 'fontsize': 16})\n","    # fill with colors\n","\n","    colors = ['pink', 'lightblue', 'lightgreen']\n","    for patch, color in zip(bplot1['boxes'], colors):\n","        patch.set_facecolor(color)\n","\n","    fig.savefig('{}/{}.png'.format(save_folder, file_name))\n","\n","    fig.show()\n","    plt.close()\n","\n","\n","def get_box_plots_mod(samples1, samples2, save_folder, file_name=None):\n","    all_data = np.transpose(np.array([samples1, samples2]))\n","    labels = ['Fake', 'Real']\n","    df = pd.DataFrame(all_data, columns=labels)\n","    import seaborn as sns\n","    import matplotlib.pyplot as plt\n","    from matplotlib import pyplot\n","\n","    fig, ax = pyplot.subplots(figsize=(3, 3.5))\n","\n","    my_pal = {\"Fake\": \"pink\", \"Real\": \"lightblue\", }\n","\n","    plt.xticks(fontsize=12)\n","    plt.yticks(fontsize=12)\n","\n","    ax = sns.boxplot(data=df, width=0.3, palette=my_pal,  showfliers=False)\n","\n","    colors = ['pink', 'lightblue']\n","    for idx,  patch in enumerate(ax.artists):\n","        r, g, b, a = patch.get_facecolor()\n","        patch.set_facecolor(colors[idx])\n","\n","    [t_val, p_val] = stats.ttest_ind(samples1, samples2, equal_var=True)\n","\n","    if p_val > 0.05:\n","        title = plt.title(file_name, fontdict={'fontweight': 'bold', 'fontsize': 16})\n","    else:\n","        plt.title(r'' + file_name + ' $\\mathbf{^{*}}$', fontdict={'fontweight': 'bold', 'fontsize': 16})\n","\n","    plt.savefig('{}/{}.png'.format(save_folder, file_name),bbox_inches=\"tight\")\n","\n","    plt.show()\n","\n","    return\n","\n","    font = {'family': 'normal',\n","            'weight': 'semibold',\n","            'size': 13}\n","    #\n","    matplotlib.rc('font', **font)\n","\n","    # plt.xlabel('l', fontsize=18)\n","    # plt.ylabel('ylabel', fontsize=16)\n","    plt.tight_layout()\n","    plt.figure(figsize=(1.5, 4))\n","\n","    fig = plt.figure(1, figsize=(1, 3), frameon=False)\n","\n","    ax1 = fig.add_subplot(111)\n","    # rectangular box plot\n","    bplot1 = ax1.boxplot(all_data,\n","                         vert=True,  # vertical box alignment\n","                         patch_artist=True,  # fill with color\n","                         labels=labels,  # will be used to label x-ticks\n","                         showfliers=False,\n","                         positions=[0, 0.5])\n","\n","    [t_val, p_val] = stats.ttest_ind(samples1, samples2, equal_var=True)\n","\n","    if p_val > 0.05:\n","        title = ax1.set_title(file_name, fontdict={'fontweight': 'bold', 'fontsize': 16})\n","    else:\n","        ax1.set_title(r'' + file_name + ' $\\mathbf{^{*}}$', fontdict={'fontweight': 'bold', 'fontsize': 16})\n","    # fill with colors\n","\n","    colors = ['pink', 'lightblue', 'lightgreen']\n","    for patch, color in zip(bplot1['boxes'], colors):\n","        patch.set_facecolor(color)\n","\n","    fig.savefig('{}/{}.png'.format(save_folder, file_name))\n","\n","    fig.show()\n","    plt.close()\n","\n","\n","if __name__ == \"__main__\":\n","    import seaborn as sns\n","\n","    all_data = np.transpose(np.array([np.random.rand(2000, ), np.random.rand(2000, )]))\n","    labels = ['Fake', 'Real']\n","    df = pd.DataFrame(all_data, columns=labels)\n","    my_pal = {\"Fake\": \"pink\", \"Real\": \"lightblue\", }\n","\n","    plt.xticks(fontsize=12)\n","    plt.yticks(fontsize=12)\n","\n","    tips = sns.load_dataset(\"tips\")\n","    ax = sns.violinplot(data=df, palette=my_pal, width=0.3, showfliers=False)\n","\n","    plt.show()\n"],"metadata":{"id":"UH7J7Q4y7-hQ","executionInfo":{"status":"ok","timestamp":1682317515572,"user_tz":-330,"elapsed":4661,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["#ANALYSIS UTIL"],"metadata":{"id":"xcNtEo3A7kZ9"}},{"cell_type":"code","source":["import errno\n","import os\n","import pickle\n","from abc import ABCMeta, abstractmethod\n","from pathlib import Path\n","\n","import numpy as np\n","from sklearn.utils import resample\n","\n","\n","class BaseFeatureHelper(metaclass=ABCMeta):\n","\n","    @abstractmethod\n","    def get_feature_group_name(self):\n","        pass\n","\n","    @abstractmethod\n","    def get_micro_feature_method_references(self):\n","        pass\n","\n","    @abstractmethod\n","    def get_micro_feature_method_names(self):\n","        pass\n","\n","    @abstractmethod\n","    def get_micro_feature_short_names(self):\n","        pass\n","\n","    @abstractmethod\n","    def get_macro_feature_method_references(self):\n","        pass\n","\n","    @abstractmethod\n","    def get_macro_feature_method_names(self):\n","        pass\n","\n","    @abstractmethod\n","    def get_macro_feature_short_names(self):\n","        pass\n","\n","    def get_dump_file_name(self, news_source, micro_features, macro_features, label, file_dir):\n","        file_tags = [news_source, label, self.get_feature_group_name()]\n","        if micro_features:\n","            file_tags.append(\"micro\")\n","\n","        if macro_features:\n","            file_tags.append(\"macro\")\n","\n","        return \"{}/{}.pkl\".format(file_dir, \"_\".join(file_tags))\n","\n","    def get_features_array(self, prop_graphs, micro_features, macro_features, news_source=None, label=None,\n","                           file_dir=\"data/features\", use_cache=False):\n","        function_refs = []\n","\n","        file_name = self.get_dump_file_name(news_source, micro_features, macro_features, label, file_dir)\n","        data_file = Path(file_name)\n","\n","        if use_cache and data_file.is_file():\n","            return pickle.load(open(file_name, \"rb\"))\n","\n","        if micro_features:\n","            function_refs.extend(self.get_micro_feature_method_references())\n","\n","        if macro_features:\n","            function_refs.extend(self.get_macro_feature_method_references())\n","\n","        if len(function_refs) == 0:\n","            return None\n","\n","        all_features = []\n","\n","        for function_reference in function_refs:\n","            features_set = get_sample_feature_value(prop_graphs, function_reference)\n","            all_features.append(features_set)\n","\n","        feature_array = np.transpose(get_numpy_array(all_features))\n","\n","        pickle.dump(feature_array, open(file_name, \"wb\"))\n","\n","        return feature_array\n","\n","    def get_feature_names(self, micro_features, macro_features):\n","        features_names = []\n","        short_feature_names = []\n","\n","        if micro_features:\n","            features_names.extend(self.get_micro_feature_method_names())\n","            short_feature_names.extend(self.get_micro_feature_short_names())\n","\n","        if macro_features:\n","            features_names.extend(self.get_macro_feature_method_names())\n","            short_feature_names.extend(self.get_macro_feature_short_names())\n","\n","        return features_names, short_feature_names\n","\n","    def print_statistics_for_all_features(self, feature_array=None, prop_graphs=None, micro_features=None,\n","                                          macro_features=None):\n","\n","        if feature_array is None:\n","            feature_array = self.get_features_array(prop_graphs, micro_features, macro_features)\n","\n","        [feature_names, short_feature_names] = self.get_feature_names(micro_features, macro_features)\n","\n","        for idx in range(len(feature_names)):\n","            feature_values = feature_array[:, idx]\n","            print_stat_values(feature_names[idx], feature_values, short_feature_names[idx])\n","\n","    def save_blox_plots_for_features(self, fake_feature_array=None, real_feature_array=None, fake_prop_graphs=None,\n","                                     real_prop_graphs=None, micro_features=None, macro_features=None, save_folder=None):\n","\n","        if fake_feature_array is None:\n","            fake_feature_array = self.get_features_array(fake_prop_graphs, micro_features, macro_features)\n","            real_feature_array = self.get_features_array(real_prop_graphs, micro_features, macro_features)\n","\n","        [feature_names, short_feature_names] = self.get_feature_names(micro_features, macro_features)\n","\n","        for idx in range(len(feature_names)):\n","            fake_feature_values = fake_feature_array[:, idx]\n","            real_feature_values = real_feature_array[:, idx]\n","\n","            get_box_plots_mod(fake_feature_values, real_feature_values, save_folder, feature_names[idx],\n","                          short_feature_names[idx])\n","\n","    def get_feature_significance_t_tests(self, fake_feature_array, real_feature_array, micro_features=None,\n","                                         macro_features=None):\n","        [feature_names, short_feature_names] = self.get_feature_names(micro_features, macro_features)\n","\n","        for idx in range(len(feature_names)):\n","            fake_feature_values = fake_feature_array[:, idx]\n","            real_feature_values = real_feature_array[:, idx]\n","            print(\"Feature {} : {}\".format(short_feature_names[idx], feature_names[idx]))\n","            perform_t_test(fake_feature_values, real_feature_values)\n","\n","    def get_feature_significance_bootstrap_tests(self, fake_feature_array, real_feature_array, micro_features=None,\n","                                                 macro_features=None):\n","\n","        [feature_names, short_feature_names] = self.get_feature_names(micro_features, macro_features)\n","\n","        for idx in range(len(feature_names)):\n","            fake_feature_values = fake_feature_array[:, idx]\n","            real_feature_values = real_feature_array[:, idx]\n","\n","            perms_fake = []\n","            perms_real = []\n","\n","            combined = np.concatenate((fake_feature_values, real_feature_values), axis=0)\n","\n","            print(\"combined shape : \", combined.shape)\n","\n","            for i in range(10000):\n","                np.random.seed(i)\n","                perms_fake.append(resample(combined, n_samples=len(fake_feature_values)))\n","                perms_real.append(resample(combined, n_samples=len(real_feature_values)))\n","\n","            dif_bootstrap_means = (np.mean(perms_fake, axis=1) - np.mean(perms_real, axis=1))\n","            print(\"diff bootstrap means : \", dif_bootstrap_means.shape)\n","\n","            obs_difs = (np.mean(fake_feature_values) - np.mean(real_feature_values))\n","\n","            p_value = dif_bootstrap_means[dif_bootstrap_means >= obs_difs].shape[0] / 10000\n","\n","            print(\"Feature {} : {}\".format(short_feature_names[idx], feature_names[idx]))\n","            print(\"t- value : {}   p-value : {}\".format(obs_difs, p_value))\n","\n","\n","def get_sample_feature_value(news_graps: list, get_feature_fun_ref):\n","    result = []\n","    for graph in news_graps:\n","        result.append(get_feature_fun_ref(graph))\n","\n","    return result\n","\n","\n","def create_dir(dir_name):\n","    if not os.path.exists(dir_name):\n","        try:\n","            os.makedirs(dir_name)\n","        except OSError as exc:  # Guard against race condition\n","            if exc.errno != errno.EEXIST:\n","                raise\n","\n","\n","\n","def get_epoch_timestamp_from_retweet(retweet):\n","    return twitter_datetime_str_to_object(retweet[\"created_at\"])\n","\n","\n","def sort_retweet_object_by_time(retweets: list):\n","    retweets.sort(key=get_epoch_timestamp_from_retweet)\n","\n","    return retweets\n","\n","\n","def get_noise_news_ids():\n","    with open(\"data/news_id_ignore_list\") as file:\n","        lines = file.readlines()\n","        return [line.strip() for line in lines]\n","\n","\n","def load_prop_graph(data_folder, news_source, news_label):\n","    news_graphs = pickle.load(open(\"{}/{}_{}_news_prop_graphs.pkl\".format(data_folder, news_source, news_label), \"rb\"))\n","    return news_graphs\n","\n","\n","def remove_prop_graph_noise(news_graphs, noise_ids):\n","    noise_ids = set(noise_ids)\n","    return [graph for graph in news_graphs if graph.tweet_id not in noise_ids]\n","\n","\n","def sort_tweet_node_object_by_created_time(tweet_nodes: list):\n","    tweet_nodes.sort(key=lambda x: x.created_time)\n","\n","    return tweet_nodes\n","\n","\n","def equal_samples(sample1, sample2):\n","    target_len = min(len(sample1), len(sample2))\n","\n","    np.random.seed(0)\n","\n","    np.random.shuffle(sample1)\n","    np.random.shuffle(sample2)\n","\n","    return sample1[:target_len], sample2[:target_len]\n","\n","\n","# def get_propagation_graphs(data_folder, news_source):\n","#     fake_propagation_graphs = load_prop_graph(data_folder, news_source, \"fake\")\n","#     real_propagation_graphs = load_prop_graph(data_folder, news_source, \"real\")\n","#\n","#     print(\"Before filtering no. of FAKE prop graphs: {}\".format(len(fake_propagation_graphs)))\n","#     print(\"Before filtering no. of REAL prop graphs: {}\".format(len(real_propagation_graphs)))\n","#\n","#     fake_propagation_graphs = remove_prop_graph_noise(fake_propagation_graphs, get_noise_news_ids())\n","#     real_propagation_graphs = remove_prop_graph_noise(real_propagation_graphs, get_noise_news_ids())\n","#\n","#     print(\"After filtering no. of FAKE prop graphs: {}\".format(len(fake_propagation_graphs)))\n","#     print(\"After filtering no. of REAL prop graphs: {}\".format(len(real_propagation_graphs)))\n","#     print(flush=True)\n","#\n","#     return fake_propagation_graphs, real_propagation_graphs\n","\n","\n","def get_numpy_array(list_of_list):\n","    np_array_lists = []\n","    for list_obj in list_of_list:\n","        np_array_lists.append(np.array(list_obj))\n","\n","    return np.array(np_array_lists)\n","\n","\n","def print_stat_values(feature_name, values, short_feature_name=\"\"):\n","    print(\"=========================================\")\n","    print(\"Feature {} : {}\".format(short_feature_name, feature_name))\n","    print(\"Min value : {}\".format(min(values)))\n","    print(\"Max value : {}\".format(max(values)))\n","    print(\"Mean value : {}\".format(np.mean(np.array(values))))\n","    print(\"=========================================\")\n"],"metadata":{"id":"Z9IEoZwc7lFt","executionInfo":{"status":"ok","timestamp":1682317633210,"user_tz":-330,"elapsed":596,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["#CONSTRUCT SAMPLE FEATURES"],"metadata":{"id":"GWPSqh0i7MXP"}},{"cell_type":"code","source":["import pickle\n","import queue\n","from pathlib import Path\n","\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","\n","\n","def get_features(news_graphs, micro_features, macro_features):\n","    temporal_features = get_all_temporal_features(news_graphs, micro_features, macro_features)\n","    structural_features = get_all_structural_features(news_graphs, micro_features, macro_features)\n","    linguistic_features = get_all_linguistic_features(news_graphs, micro_features, macro_features)\n","\n","    sample_features = np.concatenate([temporal_features, structural_features, linguistic_features], axis=1)\n","    return sample_features\n","\n","\n","def get_dataset(news_source, load_dataset=False, micro_features=True, macro_features=True):\n","    if load_dataset:\n","        sample_features = pickle.load(open(\"{}_samples_features.pkl\".format(news_source), \"rb\"))\n","        target_labels = pickle.load(open(\"{}_target_labels.pkl\".format(news_source), \"rb\"))\n","\n","    else:\n","        fake_prop_graph, real_prop_graph = get_nx_propagation_graphs(news_source)\n","        fake_prop_graph, real_prop_graph = equal_samples(fake_prop_graph, real_prop_graph)\n","\n","        print(\"fake samples len : {} real samples len : {}\".format(len(fake_prop_graph), len(real_prop_graph)))\n","\n","        fake_news_samples = get_features(fake_prop_graph, micro_features, macro_features)\n","        real_news_samples = get_features(real_prop_graph, micro_features, macro_features)\n","\n","        print(\"Fake feature array \")\n","        print(fake_news_samples.shape)\n","\n","        print(\"real feature array\")\n","        print(real_news_samples.shape)\n","\n","        sample_features = np.concatenate([fake_news_samples, real_news_samples], axis=0)\n","        target_labels = np.concatenate([np.ones(len(fake_news_samples)), np.zeros(len(real_news_samples))], axis=0)\n","\n","        pickle.dump(sample_features, (open(\"{}_samples_features.pkl\".format(news_source), \"wb\")))\n","        pickle.dump(target_labels, (open(\"{}_target_labels.pkl\".format(news_source), \"wb\")))\n","\n","    return sample_features, target_labels\n","\n","\n","def get_train_test_split(samples_features, target_labels):\n","    X_train, X_test, y_train, y_test = train_test_split(samples_features, target_labels, stratify=target_labels,\n","                                                        test_size=0.2, random_state=42)\n","    return X_train, X_test, y_train, y_test\n","\n","\n","def perform_pca(train_data, target_labels):\n","    pca = PCA(n_components=min(20, len(train_data[0])))\n","    pca.fit(train_data, target_labels)\n","    return pca\n","\n","\n","def get_dataset_file_name(file_dir, news_source, include_micro=True, include_macro=True, include_structural=True,\n","                          include_temporal=True,\n","                          include_linguistic=True):\n","    file_names = [news_source]\n","    if include_micro:\n","        file_names.append(\"micro\")\n","\n","    if include_macro:\n","        file_names.append(\"macro\")\n","\n","    if include_structural:\n","        file_names.append(\"struct\")\n","\n","    if include_temporal:\n","        file_names.append(\"temp\")\n","\n","    if include_linguistic:\n","        file_names.append(\"linguistic\")\n","\n","    return \"{}/{}.pkl\".format(file_dir, \"_\".join(file_names))\n","\n","\n","def get_TPNF_dataset(out_dir, news_source, include_micro=True, include_macro=True, include_structural=None,\n","                     include_temporal=None,\n","                     include_linguistic=None, time_interval=None, use_cache=False):\n","    file_name = get_dataset_file_name(out_dir, news_source, include_micro, include_macro, include_structural,\n","                                      include_temporal, include_linguistic)\n","\n","    data_file = Path(file_name)\n","\n","    if use_cache and data_file.is_file():\n","        return pickle.load(open(file_name, \"rb\"))\n","\n","    else:\n","        fake_sample_features, real_sample_features = get_dataset_feature_array(news_source, include_micro,\n","                                                                               include_macro, include_structural,\n","                                                                               include_temporal, include_linguistic,\n","                                                                               time_interval)\n","\n","        sample_features = np.concatenate([fake_sample_features, real_sample_features], axis=0)\n","        pickle.dump(sample_features, open(file_name, \"wb\"))\n","\n","        return sample_features\n","\n","\n","def get_dataset_feature_names(include_micro=True, include_macro=True, include_structural=None,\n","                              include_temporal=None,\n","                              include_linguistic=None):\n","    feature_helpers = []\n","\n","    if include_structural:\n","        feature_helpers.append(StructureFeatureHelper())\n","\n","    if include_temporal:\n","        feature_helpers.append(TemporalFeatureHelper())\n","\n","    if include_linguistic:\n","        feature_helpers.append(LinguisticFeatureHelper())\n","\n","    feature_names_all = []\n","    short_feature_names_all = []\n","\n","    for idx, feature_helper in enumerate(feature_helpers):\n","        features_names, short_feature_names = feature_helper.get_feature_names(include_micro, include_macro)\n","\n","        feature_names_all.extend(features_names)\n","        short_feature_names_all.extend(short_feature_names)\n","\n","    return feature_names_all, short_feature_names_all\n","\n","\n","def is_valid_graph(prop_graph: tweet_node, retweet=True, reply=True):\n","    \"\"\" Check if the prop graph has alteast one retweet or reply\"\"\"\n","\n","    for post_node in prop_graph.children:\n","        if (retweet and len(post_node.reply_children) > 0) or (reply and len(post_node.retweet_children) > 0):\n","            return True\n","\n","    return False\n","\n","\n","def remove_node_by_time(graph: tweet_node, limit_time):\n","    start_time = get_first_post_time(graph)\n","    end_time = start_time + limit_time\n","\n","    q = queue.Queue()\n","\n","    q.put(graph)\n","\n","    while q.qsize() != 0:\n","        node = q.get()\n","\n","        children = node.children\n","\n","        retweet_children = set(node.retweet_children)\n","        reply_children = set(node.reply_children)\n","\n","        for child in children.copy():\n","\n","            if child.created_time <= end_time:\n","                q.put(child)\n","            else:\n","                node.children.remove(child)\n","                try:\n","                    retweet_children.remove(child)\n","                except KeyError:  # Element not found in the list\n","                    pass\n","                try:\n","                    reply_children.remove(child)\n","                except KeyError:  # Element not found in the list\n","                    pass\n","\n","        node.retweet_children = list(retweet_children)\n","        node.reply_children = list(reply_children)\n","\n","    return graph\n","\n","\n","def filter_propagation_graphs(graphs, limit_time):\n","    result_graphs = []\n","\n","    for prop_graph in graphs:\n","        filtered_prop_graph = remove_node_by_time(prop_graph, limit_time)\n","        if is_valid_graph(filtered_prop_graph):\n","            result_graphs.append(filtered_prop_graph)\n","\n","    return result_graphs\n","\n","\n","def get_nx_propagation_graphs(data_folder, news_source):\n","    fake_propagation_graphs = load_from_nx_graphs(data_folder, news_source, \"fake\")\n","    real_propagation_graphs = load_from_nx_graphs(data_folder, news_source, \"real\")\n","\n","    return fake_propagation_graphs, real_propagation_graphs\n","\n","\n","def get_dataset_feature_array(news_source, include_micro=True, include_macro=True, include_structural=None,\n","                              include_temporal=None,\n","                              include_linguistic=None, time_interval=None):\n","    fake_prop_graph, real_prop_graph = get_nx_propagation_graphs(\"data/nx_network_data\", news_source)\n","\n","    fake_prop_graph, real_prop_graph = equal_samples(fake_prop_graph, real_prop_graph)\n","\n","    if time_interval is not None:\n","        time_limit = time_interval * 60 * 60\n","\n","        print(\"Time limit in seconds : {}\".format(time_limit))\n","\n","        fake_prop_graph = filter_propagation_graphs(fake_prop_graph, time_limit)\n","        real_prop_graph = filter_propagation_graphs(real_prop_graph, time_limit)\n","\n","        print(\"After time based filtering \")\n","        print(\"No. of fake samples : {}  No. of real samples: {}\".format(len(fake_prop_graph), len(real_prop_graph)))\n","\n","        fake_prop_graph, real_prop_graph = equal_samples(fake_prop_graph, real_prop_graph)\n","\n","    feature_helpers = []\n","    feature_group_names = []\n","\n","    if include_structural:\n","        feature_helpers.append(StructureFeatureHelper())\n","        feature_group_names.append(\"Structural\")\n","\n","    if include_temporal:\n","        feature_helpers.append(TemporalFeatureHelper())\n","        feature_group_names.append(\"Temporal\")\n","\n","    if include_linguistic:\n","        feature_helpers.append(LinguisticFeatureHelper())\n","        feature_group_names.append(\"Linguistic\")\n","\n","    fake_feature_all = []\n","    real_feature_all = []\n","    for idx, feature_helper in enumerate(feature_helpers):\n","        fake_features = feature_helper.get_features_array(fake_prop_graph, micro_features=include_micro,\n","                                                          macro_features=include_macro, news_source=news_source,\n","                                                          label=\"fake\")\n","        real_features = feature_helper.get_features_array(real_prop_graph, micro_features=include_micro,\n","                                                          macro_features=include_macro, news_source=news_source,\n","                                                          label=\"real\")\n","\n","        feature_names = feature_helper.get_feature_names(micro_features=include_micro, macro_features=include_macro)\n","        print(feature_names)\n","        if fake_features is not None and real_features is not None:\n","            fake_feature_all.append(fake_features)\n","            real_feature_all.append(real_features)\n","\n","            print(\"Feature group : {}\".format(feature_group_names[idx]))\n","            print(len(fake_features))\n","            print(len(real_features), flush=True)\n","\n","    return np.concatenate(fake_feature_all, axis=1), np.concatenate(real_feature_all, axis=1)\n","\n","\n","def get_dataset_statistics(news_source):\n","    fake_prop_graph, real_prop_graph = get_nx_propagation_graphs(\"data/saved_new_no_filter\", news_source)\n","\n","    fake_prop_graph, real_prop_graph = equal_samples(fake_prop_graph, real_prop_graph)\n","\n","    feature_helpers = [StructureFeatureHelper(), TemporalFeatureHelper(), LinguisticFeatureHelper()]\n","    feature_group_names = [\"StructureFeatureHelper\", \"TemporalFeatureHelper\", \"LinguisticFeatureHelper\"]\n","\n","    for idx, feature_helper in enumerate(feature_helpers):\n","        print(\"Feature group : {}\".format(feature_group_names[idx]))\n","\n","        fake_features = feature_helper.get_features_array(fake_prop_graph, micro_features=True,\n","                                                          macro_features=True, news_source=news_source, label=\"fake\")\n","        real_features = feature_helper.get_features_array(real_prop_graph, micro_features=True,\n","                                                          macro_features=True, news_source=news_source, label=\"real\")\n","\n","        feature_helper.save_blox_plots_for_features(fake_feature_array=fake_features,\n","                                                    real_feature_array=real_features, micro_features=True,\n","                                                    macro_features=True,\n","                                                    save_folder=\"data/feature_images/{}\".format(news_source))\n","\n","        feature_helper.get_feature_significance_t_tests(fake_features, real_features, micro_features=True,\n","                                                        macro_features=True)\n","\n","        # Print the statistics of the dataset\n","        print(\"------------Fake------------\")\n","        feature_helper.print_statistics_for_all_features(feature_array=fake_features, prop_graphs=fake_prop_graph,\n","                                                         micro_features=True, macro_features=True)\n","\n","        print(\"------------Real------------\")\n","        feature_helper.print_statistics_for_all_features(feature_array=real_features, prop_graphs=fake_prop_graph,\n","                                                         micro_features=True, macro_features=True)\n","\n","\n","if __name__ == \"__main__\":\n","    get_dataset_statistics(\"politifact\")\n","    get_dataset_statistics(\"gossipcop\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"N62cEGbq7Ked","executionInfo":{"status":"error","timestamp":1682317837463,"user_tz":-330,"elapsed":2130,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}},"outputId":"af36f5a8-2e30-40ac-d34f-4284669e5f91"},"execution_count":24,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-6649d196fafb>\u001b[0m in \u001b[0;36m<cell line: 288>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mget_dataset_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"politifact\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0mget_dataset_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gossipcop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-6649d196fafb>\u001b[0m in \u001b[0;36mget_dataset_statistics\u001b[0;34m(news_source)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mfake_prop_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_prop_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nx_propagation_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/saved_new_no_filter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mfake_prop_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_prop_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mequal_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_prop_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_prop_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-6649d196fafb>\u001b[0m in \u001b[0;36mget_nx_propagation_graphs\u001b[0;34m(data_folder, news_source)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_nx_propagation_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mfake_propagation_graphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_nx_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fake\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mreal_propagation_graphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_nx_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"real\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-b24a81d52064>\u001b[0m in \u001b[0;36mload_from_nx_graphs\u001b[0;34m(dataset_dir, news_source, news_label)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_dataset_sample_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/sample_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_dataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mtweet_node_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstruct_tweet_node_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/saved_new_no_filter/politifact_fake/politifact14795.json'"]}]},{"cell_type":"markdown","source":["#BASIC MODEL"],"metadata":{"id":"mc_J8SoO62HG"}},{"cell_type":"code","source":["import time\n","\n","import matplotlib\n","import numpy as np\n","from sklearn import preprocessing, svm\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","\n","\n","def get_classifier_by_name(classifier_name):\n","    if classifier_name == \"GaussianNB\":\n","        return GaussianNB()\n","    elif classifier_name == \"LogisticRegression\":\n","        return LogisticRegression(solver='lbfgs')\n","    elif classifier_name == \"DecisionTreeClassifier\":\n","        return DecisionTreeClassifier()\n","    elif classifier_name == \"RandomForestClassifier\":\n","        return RandomForestClassifier(n_estimators=50)\n","    elif classifier_name == \"SVM -linear kernel\":\n","        return svm.SVC(kernel='linear')\n","\n","\n","def train_model(classifier_name, X_train, X_test, y_train, y_test):\n","    accuracy_values = []\n","    precision_values = []\n","    recall_values = []\n","    f1_score_values = []\n","\n","    for i in range(5):\n","        classifier_clone = get_classifier_by_name(classifier_name)\n","        classifier_clone.fit(X_train, y_train)\n","\n","        predicted_output = classifier_clone.predict(X_test)\n","        accuracy, precision, recall, f1_score_val = get_metrics(y_test, predicted_output, one_hot_rep=False)\n","\n","        accuracy_values.append(accuracy)\n","        precision_values.append(precision)\n","        recall_values.append(recall)\n","        f1_score_values.append(f1_score_val)\n","\n","    print_metrics(np.mean(accuracy_values), np.mean(precision_values), np.mean(recall_values), np.mean(f1_score_values))\n","\n","\n","def print_metrics(accuracy, precision, recall, f1_score_val):\n","    print(\"Accuracy : {}\".format(accuracy))\n","    print(\"Precision : {}\".format(precision))\n","    print(\"Recall : {}\".format(recall))\n","    print(\"F1 : {}\".format(f1_score_val))\n","\n","\n","def get_metrics(target, logits, one_hot_rep=True):\n","    \"\"\"\n","    Two numpy one hot arrays\n","    :param target:\n","    :param logits:\n","    :return:\n","    \"\"\"\n","\n","    if one_hot_rep:\n","        label = np.argmax(target, axis=1)\n","        predict = np.argmax(logits, axis=1)\n","    else:\n","        label = target\n","        predict = logits\n","\n","    accuracy = accuracy_score(label, predict)\n","\n","    precision = precision_score(label, predict)\n","    recall = recall_score(label, predict)\n","    f1_score_val = f1_score(label, predict)\n","\n","    return accuracy, precision, recall, f1_score_val\n","\n","\n","def get_basic_model_results(X_train, X_test, y_train, y_test):\n","    scaler = preprocessing.StandardScaler().fit(X_train)\n","\n","    X_train = scaler.transform(X_train)\n","    X_test = scaler.transform(X_test)\n","\n","    classifiers = [GaussianNB(), LogisticRegression(), DecisionTreeClassifier(),\n","                   RandomForestClassifier(n_estimators=100),\n","                   svm.SVC()]\n","    classifier_names = [\"GaussianNB\", \"LogisticRegression\", \"DecisionTreeClassifier\", \"RandomForestClassifier\",\n","                        \"SVM -linear kernel\"]\n","\n","    for idx in range(len(classifiers)):\n","        print(\"======={}=======\".format(classifier_names[idx]))\n","        train_model(classifier_names[idx], X_train, X_test, y_train, y_test)\n","\n","\n","def get_classificaton_results_tpnf(data_dir, news_source, time_interval, use_cache=False):\n","    include_micro = True\n","    include_macro = True\n","\n","    include_structural = True\n","    include_temporal = True\n","    include_linguistic = True\n","\n","    sample_feature_array = get_TPNF_dataset(data_dir, news_source, include_micro, include_macro, include_structural,\n","                                            include_temporal, include_linguistic, time_interval, use_cache=use_cache)\n","\n","    print(\"Sample feature array dimensions\")\n","    print(sample_feature_array.shape, flush=True)\n","\n","    num_samples = int(len(sample_feature_array) / 2)\n","    target_labels = np.concatenate([np.ones(num_samples), np.zeros(num_samples)], axis=0)\n","\n","    X_train, X_test, y_train, y_test = get_train_test_split(sample_feature_array, target_labels)\n","    get_basic_model_results(X_train, X_test, y_train, y_test)\n","\n","\n","def plot_feature_importances(coef, names):\n","    imp = coef\n","    imp, names = zip(*sorted(zip(imp, names)))\n","    plt.barh(range(len(names)), imp, align='center')\n","    plt.yticks(range(len(names)), names)\n","\n","    plt.savefig('feature_importance.png', bbox_inches='tight')\n","    plt.show()\n","\n","\n","def dump_random_forest_feature_importance(data_dir, news_source):\n","    include_micro = True\n","    include_macro = True\n","\n","    include_structural = True\n","    include_temporal = True\n","    include_linguistic = True\n","\n","    sample_feature_array = get_TPNF_dataset(data_dir, news_source, include_micro, include_macro, include_structural,\n","                                            include_temporal, include_linguistic, use_cache=True)\n","\n","    sample_feature_array = sample_feature_array[:, :-1]\n","    feature_names, short_feature_names = get_dataset_feature_names(include_micro, include_macro, include_structural,\n","                                                                   include_temporal, include_linguistic)\n","\n","    feature_names = feature_names[:-1]\n","    short_feature_names = short_feature_names[:-1]\n","    num_samples = int(len(sample_feature_array) / 2)\n","    target_labels = np.concatenate([np.ones(num_samples), np.zeros(num_samples)], axis=0)\n","\n","    X_train, X_test, y_train, y_test = get_train_test_split(sample_feature_array, target_labels)\n","\n","    # Build a forest and compute the feature importances\n","    forest = ExtraTreesClassifier(n_estimators=100, random_state=0)\n","\n","    forest.fit(X_train, y_train)\n","    importances = forest.feature_importances_\n","    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n","                 axis=0)\n","    indices = np.argsort(importances)[::-1]\n","\n","    # Print the feature ranking\n","    print(\"Feature ranking:\")\n","\n","    for f in range(X_train.shape[1]):\n","        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n","\n","    matplotlib.rcParams['figure.figsize'] = 5, 2\n","\n","    # Plot the feature importances of the forest\n","    plt.figure()\n","\n","    plt.bar(range(X_train.shape[1]), importances[indices],\n","            color=\"b\", yerr=std[indices], align=\"center\")\n","    plt.xticks(range(X_train.shape[1]), np.array(short_feature_names)[indices], rotation=75, fontsize=9.5)\n","    plt.xlim([-1, X_train.shape[1]])\n","    plt.savefig('{}_feature_importance.png'.format(news_source), bbox_inches='tight')\n","\n","    plt.show()\n","\n","\n","def get_classificaton_results_tpnf_by_time(news_source: str):\n","    time_intervals = [3, 6, 12, 24, 36, 48, 60, 72, 84, 96]\n","\n","    for time_interval in time_intervals:\n","        print(\"=============Time Interval : {}  ==========\".format(time_interval))\n","        start_time = time.time()\n","        get_classificaton_results_tpnf(\"data/features\", news_source, time_interval)\n","\n","        print(\"\\n\\n================Exectuion time - {} ==================================\\n\".format(\n","            time.time() - start_time))\n","\n","\n","if __name__ == \"__main__\":\n","    get_classificaton_results_tpnf(\"data/features\", \"politifact\", time_interval=None, use_cache=False)\n","\n","    get_classificaton_results_tpnf(\"data/features\", \"gossipcop\", time_interval=None, use_cache=False)\n","\n","    # Filter the graphs by time interval (for early fake news detection) and get the classification results\n","    # get_classificaton_results_tpnf_by_time(\"politifact\")\n","    # get_classificaton_results_tpnf_by_time(\"gossipcop\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"XnHjsJPv649q","executionInfo":{"status":"error","timestamp":1682317332227,"user_tz":-330,"elapsed":437,"user":{"displayName":"IITG Rhinos","userId":"16287647213871364715"}},"outputId":"996aabbf-7908-42ea-a0ee-e44ee180b3fe"},"execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-5cb2fe0f24ab>\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mget_classificaton_results_tpnf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"politifact\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mget_classificaton_results_tpnf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gossipcop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-5cb2fe0f24ab>\u001b[0m in \u001b[0;36mget_classificaton_results_tpnf\u001b[0;34m(data_dir, news_source, time_interval, use_cache)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0minclude_linguistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     sample_feature_array = get_TPNF_dataset(data_dir, news_source, include_micro, include_macro, include_structural,\n\u001b[0m\u001b[1;32m    107\u001b[0m                                             include_temporal, include_linguistic, time_interval, use_cache=use_cache)\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_TPNF_dataset' is not defined"]}]}]}