{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MOUNT AT DRIVE"
      ],
      "metadata": {
        "id": "hwu6_iCmncC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iUeGiiii_2VP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "cc9ac93d-f40e-4637-8130-9b563c66435e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   )\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    102\u001b[0m     ):\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "A19s4PBhG6vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORTS"
      ],
      "metadata": {
        "id": "cjxXXsZtnfgQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW_cMzpvGKZD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import copy\n",
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import defaultdict\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "8yFen3aIGzib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING DATA"
      ],
      "metadata": {
        "id": "GCvdnACLnljc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_tweets = []\n",
        "scraped_tweets = []\n",
        "targets = []\n",
        "\n",
        "cnt_true = 0\n",
        "cnt_false = 0\n",
        "\n",
        "\n",
        "for filename in os.listdir('true15'):\n",
        "    f = os.path.join('true15', filename)\n",
        "    cnt_true += 1\n",
        "    with open(f, mode ='r') as file:\n",
        "      csvFile = csv.reader(file)\n",
        "      cnt = 0\n",
        "      scrap_row = []\n",
        "      for row in csvFile:\n",
        "        cnt += 1\n",
        "        if(cnt==1):\n",
        "          original_tweets.append(row[0])\n",
        "        else:\n",
        "          scrap_row.append(row[0])\n",
        "\n",
        "      scraped_tweets.append(scrap_row)\n",
        "      targets.append(1)\n",
        "\n",
        "for filename in os.listdir('false15'):\n",
        "    f = os.path.join('false15', filename)\n",
        "    cnt_false += 1\n",
        "    with open(f, mode ='r') as file:\n",
        "      csvFile = csv.reader(file)\n",
        "      cnt = 0\n",
        "      scrap_row = []\n",
        "      for row in csvFile:\n",
        "        cnt += 1\n",
        "        if(cnt==1):\n",
        "          original_tweets.append(row[0])\n",
        "        else:\n",
        "          scrap_row.append(row[0])\n",
        "\n",
        "      scraped_tweets.append(scrap_row)\n",
        "      targets.append(0)\n",
        "\n",
        "print(\"cnt_true: \",cnt_true)\n",
        "print(\"cnt_false: \",cnt_false)"
      ],
      "metadata": {
        "id": "Jp_IYFY0pvt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f551f92f-ba93-4545-9916-2e7171dbd743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnt_true:  316\n",
            "cnt_false:  443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TEST DATA"
      ],
      "metadata": {
        "id": "ABFb9_4Unopq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "indexes = []\n",
        "for i in range(0,len(original_tweets)):\n",
        "  indexes.append(i)\n",
        "\n",
        "test_original_tweets = []\n",
        "test_scraped_tweets = []\n",
        "test_targets = []\n",
        "\n",
        "train_original_tweets = []\n",
        "train_scraped_tweets = []\n",
        "train_targets = []\n",
        "\n",
        "random.shuffle(indexes)\n",
        "print(indexes)\n",
        "\n",
        "for i in range(0,100):\n",
        "  test_original_tweets.append(original_tweets[indexes[i]])\n",
        "  test_scraped_tweets.append(scraped_tweets[indexes[i]])\n",
        "  test_targets.append(targets[indexes[i]])\n",
        "\n",
        "for i in range(100,len(indexes)):\n",
        "  train_original_tweets.append(original_tweets[indexes[i]])\n",
        "  train_scraped_tweets.append(scraped_tweets[indexes[i]])\n",
        "  train_targets.append(targets[indexes[i]])\n",
        "\n",
        "print((test_targets))\n",
        "print((train_targets))\n"
      ],
      "metadata": {
        "id": "d5rsM2Lom0qA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6024d1a-5609-4509-a66b-591594d1fb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[98, 39, 176, 550, 334, 476, 124, 326, 573, 595, 537, 620, 542, 346, 658, 690, 425, 464, 715, 511, 57, 73, 396, 755, 76, 30, 671, 599, 50, 545, 653, 87, 509, 446, 155, 622, 517, 180, 632, 330, 628, 659, 34, 329, 374, 24, 78, 5, 639, 246, 283, 138, 443, 79, 51, 101, 381, 302, 520, 52, 507, 89, 255, 242, 614, 623, 226, 546, 340, 705, 414, 316, 333, 386, 267, 323, 692, 651, 207, 638, 565, 411, 260, 725, 392, 120, 131, 108, 706, 0, 703, 278, 730, 288, 197, 337, 127, 23, 367, 522, 580, 371, 42, 125, 217, 404, 630, 554, 691, 469, 332, 435, 481, 13, 130, 208, 118, 531, 461, 166, 681, 709, 722, 492, 519, 629, 669, 579, 153, 585, 14, 468, 530, 336, 320, 137, 678, 276, 493, 313, 366, 317, 350, 589, 541, 401, 75, 714, 665, 454, 188, 422, 605, 674, 354, 177, 438, 719, 331, 349, 195, 35, 524, 684, 433, 666, 572, 502, 53, 408, 734, 296, 221, 462, 536, 47, 186, 499, 324, 607, 284, 380, 341, 289, 345, 148, 156, 465, 295, 338, 12, 717, 568, 416, 695, 110, 490, 68, 655, 451, 549, 483, 135, 523, 442, 497, 311, 198, 491, 402, 575, 729, 596, 215, 697, 290, 151, 9, 602, 407, 745, 275, 225, 223, 676, 143, 539, 756, 598, 61, 347, 696, 584, 229, 680, 453, 439, 16, 82, 704, 420, 758, 140, 206, 643, 495, 731, 558, 500, 586, 718, 306, 532, 69, 266, 71, 304, 516, 271, 234, 49, 167, 318, 377, 214, 412, 594, 613, 364, 631, 735, 688, 648, 383, 232, 569, 672, 169, 17, 373, 83, 297, 348, 505, 164, 445, 7, 687, 365, 218, 634, 262, 560, 534, 424, 686, 351, 245, 119, 10, 204, 436, 64, 663, 649, 303, 8, 247, 694, 693, 123, 711, 475, 18, 46, 103, 501, 286, 427, 372, 314, 116, 457, 216, 656, 33, 698, 612, 172, 437, 540, 473, 487, 239, 739, 77, 570, 503, 747, 559, 62, 413, 175, 25, 444, 415, 388, 305, 544, 113, 479, 264, 224, 107, 152, 409, 294, 253, 91, 29, 144, 72, 126, 243, 309, 335, 485, 736, 41, 258, 506, 712, 165, 285, 379, 606, 528, 142, 640, 458, 609, 391, 20, 359, 358, 525, 515, 733, 106, 527, 478, 203, 615, 328, 647, 482, 182, 257, 238, 40, 93, 122, 121, 56, 37, 724, 222, 740, 417, 574, 282, 510, 418, 548, 96, 227, 581, 741, 702, 43, 405, 3, 209, 644, 310, 157, 178, 641, 28, 489, 357, 710, 237, 562, 27, 535, 552, 728, 343, 753, 270, 132, 149, 301, 466, 173, 375, 129, 112, 757, 637, 496, 557, 55, 744, 111, 477, 463, 471, 147, 99, 81, 280, 592, 480, 441, 626, 538, 58, 661, 700, 2, 352, 292, 63, 60, 84, 707, 399, 133, 561, 189, 287, 563, 440, 319, 19, 668, 390, 293, 754, 564, 621, 59, 95, 467, 749, 387, 650, 100, 344, 657, 518, 368, 74, 452, 38, 263, 498, 752, 256, 85, 86, 385, 36, 235, 682, 748, 746, 567, 261, 259, 689, 339, 521, 369, 398, 551, 716, 117, 617, 363, 1, 249, 421, 268, 543, 240, 667, 556, 513, 616, 161, 723, 577, 196, 181, 428, 210, 410, 488, 22, 321, 6, 15, 533, 92, 727, 618, 588, 673, 652, 737, 406, 382, 128, 486, 426, 512, 378, 434, 279, 397, 664, 571, 307, 104, 299, 219, 459, 484, 591, 163, 474, 114, 325, 191, 185, 699, 504, 362, 250, 423, 619, 566, 393, 472, 201, 90, 190, 145, 308, 45, 450, 448, 353, 603, 202, 315, 168, 228, 547, 604, 115, 376, 97, 193, 244, 109, 159, 231, 713, 750, 281, 160, 590, 456, 449, 200, 254, 597, 701, 742, 342, 199, 4, 455, 732, 184, 679, 389, 660, 683, 738, 170, 361, 32, 721, 587, 298, 211, 431, 645, 273, 65, 646, 141, 48, 146, 432, 272, 66, 187, 194, 94, 162, 633, 213, 403, 248, 600, 277, 265, 236, 192, 677, 88, 636, 582, 635, 230, 158, 708, 102, 322, 751, 743, 460, 447, 642, 139, 183, 11, 675, 430, 670, 370, 26, 400, 601, 220, 179, 355, 70, 514, 470, 171, 241, 252, 205, 685, 610, 212, 312, 136, 356, 251, 555, 611, 625, 105, 593, 269, 608, 360, 44, 31, 494, 526, 300, 419, 150, 429, 576, 529, 384, 624, 327, 174, 578, 291, 627, 274, 654, 720, 662, 726, 54, 21, 508, 394, 80, 553, 154, 134, 233, 583, 67, 395]\n",
            "[1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
            "[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CODE"
      ],
      "metadata": {
        "id": "QqWTf_oZntJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, original_tweets, scraped_tweets, targets, tokenizer, max_length=128):\n",
        "        self.max_len = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tweets = original_tweets\n",
        "        self.scraped_tweets = scraped_tweets\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.tweets[index]\n",
        "        scraped = self.scraped_tweets[index]\n",
        "        all_tweets = []\n",
        "        all_tweets.append(text)\n",
        "        all_tweets.extend(scraped)\n",
        "        inputs = self.tokenizer(\n",
        "                        all_tweets,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        add_special_tokens=True,\n",
        "                        max_length=self.max_len,\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "        original_tweet = {\n",
        "            'input_ids': inputs['input_ids'][0],\n",
        "            'attention_mask': inputs['attention_mask'][0],\n",
        "        }\n",
        "        tweets = {\n",
        "            'input_ids': inputs['input_ids'][1:],\n",
        "            'attention_mask': inputs['attention_mask'][1:],\n",
        "        }\n",
        "        return {\n",
        "            'original_tweet': original_tweet,\n",
        "            'tweets': tweets,\n",
        "            'target': self.targets[index]\n",
        "        }"
      ],
      "metadata": {
        "id": "uJwDK-f2pbZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = FakeNewsDataset(train_original_tweets,\n",
        "                     train_scraped_tweets,\n",
        "                     train_targets,\n",
        "                     tokenizer)\n",
        "valid_ds = FakeNewsDataset(test_original_tweets,\n",
        "                     test_scraped_tweets,\n",
        "                     test_targets,\n",
        "                     tokenizer)"
      ],
      "metadata": {
        "id": "-ItaKOrQr21H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(Model, self).__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
        "        self.attn_head = nn.MultiheadAttention(embed_dim=self.config.hidden_size, num_heads=8)\n",
        "        self.qkv = nn.Linear(self.config.hidden_size, 3*self.config.hidden_size)\n",
        "        self.head = nn.Linear(2*self.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, original_tweet, tweets):\n",
        "        out1 = self.model(input_ids=original_tweet['input_ids'],\n",
        "                         attention_mask=original_tweet['attention_mask'])\n",
        "        out2 = self.model(input_ids=tweets['input_ids'].squeeze(0),\n",
        "                         attention_mask=tweets['attention_mask'].squeeze(0))\n",
        "        query = self.qkv(out1.pooler_output)[:, :self.config.hidden_size]\n",
        "        out = self.qkv(out2.pooler_output)\n",
        "        key, value = out[:, self.config.hidden_size: 2*self.config.hidden_size], out[:, 2*self.config.hidden_size: 3*self.config.hidden_size]\n",
        "        attn_output, attn_output_weights = self.attn_head(query, key, value)\n",
        "        concat_vector = torch.concat([out1.pooler_output, attn_output], dim=1)\n",
        "        output = self.head(concat_vector)\n",
        "        return output"
      ],
      "metadata": {
        "id": "hv5Y0LRJjDBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "N_ACCUMULATE = 16\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "KYsa4chsArot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(outputs, labels):\n",
        "    return nn.BCEWithLogitsLoss()(outputs.view(-1), labels.float().view(-1))"
      ],
      "metadata": {
        "id": "zxaVDjNlAhT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "    model.train()\n",
        "    model.model.eval()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:\n",
        "        original_tweet = {k: v.to(device) for k, v in data['original_tweet'].items()}\n",
        "        tweets = {k: v.to(device) for k, v in data['tweets'].items()}\n",
        "        # print(tweets['input_ids'].shape)\n",
        "        target = data['target'].to(device)\n",
        "\n",
        "        batch_size = len(original_tweet)\n",
        "\n",
        "        outputs = model(original_tweet, tweets)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss = loss / N_ACCUMULATE\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % N_ACCUMULATE == 0:\n",
        "            optimizer.step()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
        "                        LR=optimizer.param_groups[0]['lr'])\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "e3Vzy1oPyoJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    LABELS = []\n",
        "    PREDS = []\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    for step, data in bar:\n",
        "        original_tweet = {k: v.to(device) for k, v in data['original_tweet'].items()}\n",
        "        tweets = {k: v.to(device) for k, v in data['tweets'].items()}\n",
        "        target = data['target'].to(device)\n",
        "\n",
        "        batch_size = len(original_tweet)\n",
        "\n",
        "        outputs = model(original_tweet, tweets)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        PREDS.append(torch.sigmoid(outputs).cpu().detach().numpy())\n",
        "        LABELS.append(target.cpu().detach().numpy())\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss)\n",
        "\n",
        "    LABELS = np.concatenate(LABELS)\n",
        "    PREDS = np.concatenate(PREDS)\n",
        "    PREDS = np.where(PREDS > 0.5, 1, 0)\n",
        "    val_acc = accuracy_score(LABELS, PREDS)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss, val_acc"
      ],
      "metadata": {
        "id": "YM_f0-eQyoGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    start = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_epoch_acc = 0\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        gc.collect()\n",
        "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler,\n",
        "                                           dataloader=train_loader,\n",
        "                                           device=device, epoch=epoch)\n",
        "\n",
        "        val_epoch_loss, val_epoch_acc = valid_one_epoch(model, valid_loader,\n",
        "                                                       device=device,\n",
        "                                                       epoch=epoch)\n",
        "\n",
        "        history['Train Loss'].append(train_epoch_loss)\n",
        "        history['Valid Loss'].append(val_epoch_loss)\n",
        "        history['Valid Acc'].append(val_epoch_acc)\n",
        "\n",
        "        # deep copy the model\n",
        "        if val_epoch_acc >= best_epoch_acc:\n",
        "            print(f\"Validation Acc Improved ({best_epoch_acc} ---> {val_epoch_acc})\")\n",
        "            best_epoch_acc = val_epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            PATH = \"Acc{:.4f}_epoch{:.0f}.bin\".format(best_epoch_acc, epoch)\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            # Save a model file from the current directory\n",
        "            print(\"Model Saved\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    end = time.time()\n",
        "    time_elapsed = end - start\n",
        "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "    print(\"Best Acc: {:.4f}\".format(best_epoch_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "WK7cMwL7yoEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model('bert-base-uncased')\n",
        "model.to(DEVICE);"
      ],
      "metadata": {
        "id": "S08Nj8d0VW_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4,\n",
        "                       weight_decay=1e-6)\n",
        "scheduler = None"
      ],
      "metadata": {
        "id": "P_J_e0eqyoB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loader"
      ],
      "metadata": {
        "id": "QGFNxvUzgb91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "oE014OX0VefT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpH389zybhpd",
        "outputId": "ebb7865d-6cf4-4ef2-dacf-a5d646ac9dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['true16',\n",
              " 'false16',\n",
              " 'false15',\n",
              " 'true15',\n",
              " 'Acc0.6800_epoch1.bin',\n",
              " 'Acc0.6900_epoch2.bin',\n",
              " 'Acc0.7100_epoch3.bin',\n",
              " 'Acc0.5300_epoch1.bin',\n",
              " 'Acc0.6400_epoch1.bin',\n",
              " 'Tweet Retrieve.ipynb',\n",
              " 'BTP Model.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RUN MODEL"
      ],
      "metadata": {
        "id": "GRYmdky5gevH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model('bert-base-uncased')\n",
        "model.load_state_dict(torch.load(\"Acc0.7100_epoch3.bin\"))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "valid_one_epoch(model, valid_loader, DEVICE, epoch=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lAaUzTF0-Jz",
        "outputId": "e0a0990b-7af1-4d43-8669-800b73245a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 100/100 [00:20<00:00,  4.95it/s, Epoch=1, Valid_Loss=0.425]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4254662098735571, 0.77)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = run_training(model, optimizer, scheduler,device=DEVICE,num_epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "IiWB_BLcyn_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "db4eb9da-4e80-4e50-ebf9-4f69c2fa54f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU: Tesla T4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 424/659 [03:21<01:51,  2.11it/s, Epoch=1, LR=0.0001, Train_Loss=0.0261]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-45d141fc59a7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-b794f1efde2b>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n\u001b[0m\u001b[1;32m     13\u001b[0m                                            \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                            device=device, epoch=epoch)\n",
            "\u001b[0;32m<ipython-input-18-d3086c98e4d2>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_ACCUMULATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mN_ACCUMULATE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}